<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Recursive Least Squares Derived from Offline Estimation | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2024/linear-model/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Recursive Least Squares Derived from Offline Estimation</h1> <p class="post-meta">June 15, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/linear-model"> <i class="fas fa-hashtag fa-sm"></i> linear_model,</a>   <a href="/blog/tag/recursive-sysid"> <i class="fas fa-hashtag fa-sm"></i> recursive_sysid</a>     ·   <a href="/blog/category/control"> <i class="fas fa-tag fa-sm"></i> control</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>An obvious approach from offline estimation to recursive (online) estimation is to take any offline method and modify it. Here we show a method of how to modify the offline least squares to recursive least squares.</p> <h2 id="linear-difference-equation">Linear Difference Equation</h2> \[\begin{align} y(t)+a_1y(t-1) + \dots + a_n y(t-n) = b_1u(t-1)+\dots+b_mu(t-m)+v(t) \label{eq:linear} \end{align}\] <p>where \(\{u(t)\},\{y(t)\}\) are input and output signals and \(v(t)\) is some disturbance of unspecified character.</p> <p>Let \(q^{-1}\) be backward shift operator, then</p> \[A(q^{-1})y(t) = B(q^{-1})u(t) + v(t),\] <p>where \(A(q^{-1})\) and \(B(q^{-1})\) are polynomials in the delay operator:</p> \[A(q^{-1}) = 1+a_1q^{-1}+\dots + a_nq^{-n}\\ B(q^{-1}) = 1+b_1q^{-1}+\dots + b_m q^{-m}\] <p>Introduce the vector of lagged input-output data (regressor),</p> \[\phi^T(t)=\begin{bmatrix} -y(t-1) &amp;\dots -y(t-n)&amp;u(t-1)&amp;\dots u(t-m) \end{bmatrix}\] <p>Then, \(~\eqref{eq:linear}\) can be rewritten as</p> \[y(t)=\theta^T\phi(t)+v(t)\] <p>where \(\theta^T=\begin{bmatrix}a_1 &amp; \dots &amp;a_n &amp;b_1 &amp;\dots b_m \end{bmatrix}^T\) is the paramter vector.</p> <p>If the character of the disturbance term \(v(t)\) is not specified, it is natural to use</p> \[\hat{y}(t\vert \theta)\triangleq\theta^T\phi(t)\] <p>as the prediction of \(y(t)\) having observed previous inputs and outputs.</p> <h2 id="offline-identification-the-least-squares">Offline Identification: The least squares</h2> <p>The parameter vector can be estimated from the measurements of \(y(t)\) and \(\phi(t)\) with $$t=1,2,\dots,N$. A common way to choose the estimation is to minimize</p> \[V_N(\theta)=\frac{1}{N}\sum_1^N \alpha_t[y(t)-\theta^T\phi(t)]^2\] <p>with respect to \(\theta\) and \(\{\alpha_t\}\) is a sequence of positive numbers allowing to give different weights to different observations. This criterion \(V_N(\theta)\) is quadratic in \(\theta\) and thus it can be minimized analytically,</p> \[\begin{align} \hat{\theta}(N) = \left[ \sum_{1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \sum_{1}^N \alpha_t\phi(t)y(t), \label{eq:offline_ls} \end{align}\] <p>where we assume that the inverse exists. It can be written in a recursive fashion. Let</p> \[\bar{R}(t) = \sum_{1}^N \alpha_t\phi(t)\phi(t)^T.\] <p>Then, from \(\eqref{eq:offline_ls}\), we can get</p> \[\sum_{1}^N \alpha_t\phi(t)y(t) = \bar{R}(t-1)\hat{\theta}(t-1).\] <p>From the definition of \(\bar{R}(t)\),</p> \[\bar{R}(t-1)=\bar{R}(t)-\alpha_t\phi(t)\phi^T(t)\] <p>Thus \(\begin{align} \hat{\theta}(t) &amp;=\bar{R}^{-1}(t)\left[ \sum_{k=1}^{t-1} \alpha_k\phi(k)y(k) + \alpha_t \phi(t)y(t) \right]\\ &amp;=\bar{R}^{-1}(t)\left[ \bar{R}(t-1)\hat{\theta}(t-1)+ \alpha_t \phi(t)y(t) \right]\\ &amp;=\bar{R}^{-1}(t)\left[ \bar{R}(t)\hat{\theta}(t-1)+ \alpha_t \phi(t)[-\phi^T(t)\hat{\theta}(t-1) + y(t)] \right]\\ &amp;=\hat{\theta}(t-1) + \bar{R}^{-1}(t)\phi(t)\alpha_t[y(t) - \hat{\theta}^T(t-1)\phi(t)] \end{align}\)</p> <p>and</p> \[\bar{R}(t)=\bar{R}(t-1) + \alpha_t\phi(t)\phi^T(t)\] <p>Sometimes we may prefer to work with</p> \[R(t) \triangleq \frac{1}{t}\bar{R}(t)\] <p>Then</p> \[R(t)=\frac{1}{t} \left [\bar{R}(t-1) + \alpha_t\phi(t)\phi^T(t)\right]=\frac{t-1}{t} R(t-1)+\frac{1}{t}\alpha_t\phi(t)\phi^T(t)\\ =R(t-1) + \frac{1}{t}[ \alpha_t\phi(t)\phi^T(t)-R(t-1) ]\] <p>In summary, we can write \(\begin{align} \hat{\theta}(t)&amp;=\hat{\theta}(t-1) + \frac{1}{t}R^{-1}(t)\phi(t)\alpha_t [y(t)-\theta^T(t-1)\phi(t)],\\ R(t)&amp;=R(t-1) + \frac{1}{t}[ \alpha_t\phi(t)\phi^T(t)-R(t-1)]. \label{eq:offline_rls1} \end{align}\)</p> <h2 id="an-equivalent-form-recursive-least-squares">An Equivalent Form: Recursive Least Squares</h2> <p>Equation \(\eqref{eq:offline_rls1}\) is not that suited for computation since a matrix inverse has to be calculated in each time step. It’s more natural to introduce</p> \[P(t)=\bar{R}^{-1}=\frac{1}{t}R^{-1}(t)\] <p>and update \(P(t)\) directly instead. This can be done by matrix inverse lemma</p> \[[A+BCD]^{-1}=A^{-1}-A^{-1}B[DA^{-1}B+C^{-1}]^{-1}DA^{-1}.\] <p>The proof can be done by multiplying the RHS by $$(A+BCD)$.</p> <p>Let \(A=P(t-1)$,\)B=\phi(t)$, \(C=\alpha_t\) and \(D=\phi^T(t)\),</p> \[\begin{align} P(t)&amp;=\left[P^{-1}(t-1)+\phi(t)\alpha_t\phi^T(t) \right]^{-1}\\ &amp;=P(t-1)-P(t-1)\phi(t) \left[ \phi^T(t)P(t-1)\phi(t)+\frac{1}{\alpha_t} \right]^{-1}\phi^T(t)P(t-1)\\ &amp;=P(t-1)-\frac{P(t-1)\phi(t)\phi^T(t)P(t-1)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)} \end{align}\] <p>Now the inversion of a square matrix of dim \(\theta\) is replaced by inversion of a scalar \(\alpha_t\).</p> <p>Thus the recursive least squares can be written as</p> \[\begin{align} \hat{\theta}(t)&amp;=\hat{\theta}(t-1) + L(t)[y(t) -\hat{\theta}^T(t-1)\phi(t)],\\ L(t)&amp;=\frac{P(t-1)\phi(t)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)},\\ P(t)&amp;=P(t-1)-\frac{P(t-1)\phi(t)\phi^T(t)P(t-1)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)}. \label{eq:rls} \end{align}\] <h3 id="initial-conditions">Initial Conditions</h3> <p>The only assumption we made is the \(\bar{R}(t)\) is invertible and typically it becomes invertible at time \(t_0=dim~ \phi(t) = dim~\theta\). Thus strictly speaking, the proper initials values for \(\eqref{eq:rls}\) are obtained if starting at the time \(t_0\) for which</p> \[P(t_0)= \left [ \sum_{k}^{t_0} \alpha_k\phi(k)\phi(k)^T \right ]^{-1} \\ \hat{\theta}(t_0) = P(t_0)\sum_{k}^{t_0}\alpha_k \phi(k)y(k).\] <p>It is more common to start at \(t=0\) with some invertible matrix \(P(0)\) and a vector $$\hat{\theta}(0)$. Then, the resulting estimates are</p> \[\hat{\theta}(t) = \left [ P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\left [ P^{-1}(0)\hat{\theta}(0)+\sum_{k=1}^t \alpha_k\phi(k)y(k) \right].\] <p>We can see that the relative importance of the initial values decays over time as the magnitudes of the sums increase. Also, as \(P^{-1}(0)\rightarrow0\), the recursive estimate goes to the offline one. A common choice of initial values is to take \(P(0)=C \cdot I\) and \(\hat{\theta}(0)=0\), where \(C\) is some large constant.</p> <h3 id="asymptotic-properties">Asymptotic Properties</h3> <p>We assume that the data are generated by</p> \[y(t) = \theta_\circ \phi(t) + v(t)\] <p>Inserting this equation to \(\eqref{eq:offline_ls}\),</p> \[\begin{align} \hat{\theta}(N) &amp;= \left[ \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \left \{ \sum_{t=1}^N \alpha_t\left[\phi(t)\phi^T(t)\theta_\circ+\phi(t)v(t)\right] \right\}\\ &amp;= \theta_\circ + \left[ \frac{1}{N} \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t) \end{align}\] <p>According to the law of large numbers, the sum \(\frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t)\) will converge to its expected values as \(N\) goes to infinity. The expected values depend on the correlation between the disturbance term \(v(t)\) and the data vector \(\phi(t)\). It’s zero only when \(v(t)\) and \(\phi(t)\) are uncorrelated. This is true when</p> <ul> <li>\(\{v(t)\}\) is i.i.d with zero means</li> <li>\(n=0\) and \(\{u(t)\}\) is independent of the zero-mean noise sequence \(\{v(t)\}\)</li> </ul> <p>In both cases, the \(\hat{\theta}(N)\) approaches to \(\theta_\circ\) as \(N\) goes to infinity.</p> <h3 id="interpretations-of-rls">Interpretations of RLS</h3> \[\begin{align} \hat{\theta}(N) &amp;= \left[ \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \left \{ \sum_{t=1}^N \alpha_t\left[\phi(t)\phi^T(t)\theta_\circ+\phi(t)v(t)\right] \right\}\\ &amp;= \theta_\circ + \left[ \frac{1}{N} \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t) \end{align}\] <ol> <li> <p>Beforehand, we have shown that how RLS derived from the offline LS version and \(\hat{\theta}(t) = \left [ P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\left [ P^{-1}(0)\hat{\theta}(0)+\sum_{k=1}^t \alpha_k\phi(k)y(k) \right]\)</p> <p>With \(P^{-1}(0)=0\), it minimizes the least sqaures criterion</p> \[V_t(\theta)=\frac{1}{t}\sum_{k=1}^t \alpha_k[y(k)-\theta^T\phi(k)]^2\] </li> <li> <p>The estimate \(\hat{\theta}\) can be seen as the Kalman filter state estimate for state-space model \(\theta(t+1) =\theta(t)\\ y(t)=\phi^T(t)\theta(t)+v(t)\)</p> </li> <li> <p>The RLS is a recursive minimization of \(\bar{V}(\theta)=\mathbb{E}\frac{1}{2}[y(t)-\theta^T\phi(t)]^2.\)</p> <p>The factor \(\phi(t)[y(t) -\hat{\theta}^T(t-1)\phi(t)]\) is then an estimate of the gradient while</p> \[t\cdot P(t) =\left [ \frac{1}{t} P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\] <p>is the inverse of an estimate of the second derivate of the criterion. The updating \(\hat{\theta}(t)\) is thus updated with the direction of “Newton” and a decaying step size \(\frac{1}{t}\). More can be found in another blog post “Recursive Least Squares Derived from Stochastic Approximation Approach “.</p> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification/">A general framework of Recursive System Identification (Part I)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Recursive-Least-Squares-Derived-from-Stochastic-Approximation-Approach/">Recursive Least Squares Derived from Stochastic Approximation Approach</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification-2/">Offline Identification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-model-based-RL/">Model-based Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-Model-free-RL/">Model-free Reinforcement Learning</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>