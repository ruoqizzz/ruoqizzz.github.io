<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A general framework of Recursive System Identification (Part II) | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2024/A-general-framework-of-Recursive-System-Identification-2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A general framework of Recursive System Identification (Part II)</h1> <p class="post-meta">June 25, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/recursive-sysid"> <i class="fas fa-hashtag fa-sm"></i> recursive_sysid</a>     ·   <a href="/blog/category/control"> <i class="fas fa-tag fa-sm"></i> control</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This blog post aims to summarise Chapter 3 of <a href=",https://mitpress.mit.edu/9780262620581/theory-and-practice-of-recursive-identification/">Theory and Practice of Recursive Identification</a> which derives a general recursive identification method that can be applied to any set of (linear) models.</p> <p>To develop a unified approach to recursive identification consists 3 phases：</p> <ol> <li>Define the framework</li> <li>Derive the Algorithm: In the book, the authors mainly focus on <strong>minimizing the prediction error variance recursively</strong>, using the idea from stochastic approximation (See blog post Recursive Least Squares Derived from Stochastic Approximation Approach for details).</li> <li>Apply the Algorithm: We will show how the general algorithm can be applied to a particular model set, <strong>a linear regression model</strong> later.</li> </ol> <p>This is the second part of this blog series, please first see <em><a href="https://ruoqizzz.github.io/blog/2024/A-general-framework-of-Recursive-System-Identification/">A general framework of Recursive System Identification (Part I): System and Models</a>.</em></p> <h2 id="recursive-gauss-newton-algorithms-for-quadratic-criteria">Recursive Gauss-Newton Algorithms for Quadratic Criteria</h2> <p>Here we will talk about how to derive a recursive algorithm for the estimation of model parameters minimizing a prediction error criterion. The development will lead to basic general algorithm that is the main object of recursive identification.</p> <p>In the spirit of the offline criterion (see blog <em><a href="">Offline Identification</a></em>), we would like to select \(\theta\) such that the criterion</p> \[\mathbb{E}_{z^t}l(t, \theta, \varepsilon(t,\theta))\] <p>where \(z^t = (y(t), u(t))\) gives the current outputs and inputs. This type of criteria can be minimized recursively using the stochastic approximation approach (see blog <em><a href="https://ruoqizzz.github.io/blog/2024/Recursive-Least-Squares-Derived-from-Stochastic-Approximation-Approach/">Recursive Least Squares Derived from Stochastic Approximation Approach</a></em> for details.) For example, for the problem</p> \[\begin{align} \min_x V(x) \\ V(x)=\mathbb{E}_e J(x, e(t)), \end{align}\] <p>it can be recursively minimized by the stochastic Newton method,</p> \[\begin{align} \hat{x}(t)=\hat{x}(t-1) + \alpha(t) \left [ \bar{V}^{''}(\hat{x}(t-1), e(t)) \right]^{-1} Q(\hat{x}(t-1), e(t)), \label{eq:stochastic_newton} \end{align}\] <p>where \(-Q(x,e)\) is the gradient of \(J(x,e)\) with respect to \(x\) and \(\bar{V}^{''}(x,e)\) is some approximation of the second derivative of \(V(x)\) based on the observations up to time \(t\).</p> <p>To be more general, here we assume that the limit</p> \[\bar{E} l(t, \theta, \varepsilon(t,\theta)) \triangleq \lim_{N \rightarrow \infty} l(t, \theta, \varepsilon(t,\theta)) \triangleq \bar{V}(\theta)\] <p>exists.</p> <h3 id="a-general-minimization-algorithm-for-quadratic-criteria">A General Minimization Algorithm for Quadratic Criteria</h3> <p>Here we talk about the special case when \(l\) is quadratic in \(\varepsilon\),</p> \[\begin{align} V(\theta) = \mathbb{E} l (t,\theta,\varepsilon)= \mathbb{E} \frac{1}{2}\varepsilon^T(t,\theta)\Lambda^{-1}\varepsilon^T(t,\theta). \label{eq:V} \end{align}\] <p>Here we have</p> \[\left[ \frac{d}{d\theta}l (t,\theta,\varepsilon)\right]^T=-\psi(t,\theta)\Lambda^{-1}\varepsilon(t,\theta)\] <p>where</p> \[\frac{d}{d\theta}\varepsilon(t,\theta)=\frac{d}{d\theta}[y(t)-\hat{y}(t\vert \theta)=-\psi^T(t,\theta).\] <p>Let’s denote the second-derivative approximation \(\bar{V}^{''}(x,e)\) by \(R(t)\), the algorithm \(\eqref{eq:stochastic_newton}\) becomes,</p> \[\begin{align} \hat{\theta}(t)=\hat{\theta}(t-1) + \alpha(t) R^{-1} \psi(t,\hat{\theta}(t-1)\Lambda^{-1}\varepsilon(t,\hat{\theta}(t-1)). \label{eq:stochastic_newton_theta} \end{align}\] <p>The quantities \(\psi(t,\hat{\theta}(t-1)\) and \(\varepsilon(t,\hat{\theta}(t-1))\) can be computed using</p> \[\begin{align} \xi (k+1, \hat{\theta}(t-1)) &amp;= A(\hat{\theta}(t-1) \xi (k, \hat{\theta}(t-1)) + B(\hat{\theta}(t-1))z(k),\\ k&amp;=0,1,\dots,t-1, \xi(0, \hat{\theta}(t-1))=\xi_0\\ \begin{pmatrix}\hat{y}(t\vert \hat{\theta}(t-1))\\ \text{col}~\psi(t,\hat{\theta}(t-1)) \end{pmatrix}&amp;=C(\hat{\theta}(t-1))\xi(t,\hat{\theta}(t-1))\\ \varepsilon(t,\hat{\theta}(t-1))&amp;=y(t)-\hat{y}(t\vert \hat{\theta}(t-1)) \label{eq:newton-kalman} \end{align}\] <p>The problem here is that \(\eqref{eq:newton-kalman}\) is not recursive in general. To compute \(\xi (k+1, \hat{\theta}(t-1))\), we need the data from \(z(0)\) to \(z(t-1)\). It can be solved by first rewrite \(\xi (k+1, \hat{\theta}(t-1))\).</p> \[\xi (k+1, \hat{\theta}(t-1)) = [A(\hat{\theta}(t-1)]^t\xi_0 + \sum_{k=0}^{t-1}[A(\hat{\theta}(t-1)]^{t-k-1}B(\hat{\theta}(t-1)z(k).\] <p>If \(\hat{\theta}(t-1)\in D_s\) where \(D_s=\{ \theta \vert A(\theta) \text{ has all eigenvalues strictly inside the unit circle} \}\), then the factor \([A(\hat{\theta}(t-1)]^t\) tends to zero exponentially. Consequently, the sum is dominated by the last terms \(k=t-K, t-K+1, \dots, t-1\) for some value \(K\). Also since \(\alpha(t)\) is a small number for large \(t\), the difference between \(\hat{\theta}(t-1)\) and \(\hat{\theta}(t-K)\) will be quite small for large \(t\). Combining these two factors,</p> \[\xi(t, \hat{\theta}(t-1)) \approx \xi(t) \triangleq \sum_{k=1}^{t-1} \left [ \sum_{s=k+1}^{t-1}A(\theta({\hat{s}))} \right] B( \hat{\theta}(k)z(k)\] <p>Then \(\xi(t)\) can be computed recursively without all history data</p> \[\begin{align} \xi (t+1)= A(\hat{\theta}(t))\xi(t) + B(\hat{\theta}(t))z(t). \label{eq:recursive_xi} \end{align}\] <p>Based on this,</p> \[\begin{pmatrix} \hat{y}(t | \hat{\theta}(t-1)) \\ \text{col } \psi(t, \hat{\theta}(t-1)) \end{pmatrix} \approx \begin{pmatrix} \hat{y}(t) \\ \text{col } \psi(t) \end{pmatrix} \triangleq C(\hat{\theta}(t-1)) \xi(t).\] <p>and</p> \[\begin{align} \varepsilon(t, \hat{\theta}(t-1)) \approx \varepsilon(t) = y(t) - \hat{y}(t). \label{eq:recursive_epsilon} \end{align}\] <p>These gives</p> \[\hat{\theta}(t) = \hat{\theta}(t-1) + \alpha(t) R^{-1}(t) \psi(t) \Lambda^{-1} \varepsilon(t).\] <p>This show shows that by using the approximations and understanding that the influence of older data diminishes exponentially, we can reformulate the original algorithm into a truly recursive one. This allows us to compute the required terms at each time step efficiently without needing the full history of data, thus making the algorithm more practical and computationally efficient.</p> <h3 id="the-gauss-newton-search-direction">The Gauss-Newton Search Direction</h3> <p>Let’s discuss how to choose \(R(t)\).</p> <p>The Hessian of \(V(\theta)\) in \(\eqref{eq:V}\) is given by \(\frac{d^2}{d\theta^2} V(\theta) = \frac{d^2}{d\theta^2} \mathbb{E} \frac{1}{2}\varepsilon^T(t,\theta)\Lambda^{-1}\varepsilon^T(t,\theta) \\ = \mathbb{E}\frac{d}{d\theta} -\varepsilon(t,\theta) \Lambda^{-1}\psi^T(t, \theta)\\ = \mathbb{E}\psi(t, \theta)\Lambda^{-1}\psi^T(t, \theta) + \mathbb{E} \left \{ \left [ \frac{d^2}{d\theta^2} \varepsilon^T(t,\theta) \right ] \Lambda^{-1} \varepsilon(t,\theta) \right \}.\)</p> <p>Here the second derivative of \(\varepsilon\) is a matrix whose \(i,j\)-component is given by</p> \[\mathbb{E} \sum_{k,l=1}^p \left[ \frac{d}{d\theta_i}\frac{d}{d\theta_j} \varepsilon_k(t,\theta)\right](\Lambda^{-1})_{kl}\varepsilon_l(t,\theta).\] <p><strong>[Example]</strong></p> \[\varepsilon(t, \theta) = \begin{pmatrix} \varepsilon_1(t, \theta) \\ \varepsilon_2(t, \theta) \end{pmatrix}\] <p>The first derivative of \(\varepsilon(t, \theta)\) with respect to \(\theta\) is:</p> \[\frac{\partial \epsilon(t, \theta)}{\partial \theta} = \begin{pmatrix} \frac{\partial \varepsilon_1(t, \theta)}{\partial \theta_1} &amp; \frac{\partial \varepsilon_1(t, \theta)}{\partial \theta_2} \\ \frac{\partial \varepsilon_2(t, \theta)}{\partial \theta_1} &amp; \frac{\partial \varepsilon_2(t, \theta)}{\partial \theta_2} \end{pmatrix} = \begin{pmatrix} t &amp; 2\theta_2 \\ t \cos(\theta_1 t) &amp; 1 \end{pmatrix}\] <p>Second Derivative of \(\varepsilon(t, \theta)\):</p> <p>For \(\varepsilon_1(t, \theta) = \theta_1 \cdot t + \theta_2^2\),</p> \[\frac{\partial^2 \varepsilon_1(t, \theta)}{\partial \theta_1^2} = 0, \quad \frac{\partial^2 \varepsilon_1(t, \theta)}{\partial \theta_1 \partial \theta_2} = 0\\ \frac{\partial^2 \varepsilon_1(t, \theta)}{\partial \theta_2 \partial \theta_1} = 0, \quad \frac{\partial^2 \varepsilon_1(t, \theta)}{\partial \theta_2^2} = 2\] <p>For \(\varepsilon_2(t, \theta) = \sin(\theta_1 \cdot t) + \theta_2\)</p> \[\frac{\partial^2 \varepsilon_2(t, \theta)}{\partial \theta_1^2} = -t^2 \sin(\theta_1 t), \quad \frac{\partial^2 \varepsilon_2(t, \theta)}{\partial \theta_1 \partial \theta_2} = 0 \\ \frac{\partial^2 \varepsilon_2(t, \theta)}{\partial \theta_2 \partial \theta_1} = 0, \quad \frac{\partial^2 \varepsilon_2(t, \theta)}{\partial \theta_2^2} = 0\] <p>So, the second derivative tensor is:</p> \[\begin{pmatrix} \frac{\partial^2 \varepsilon_1}{\partial \theta_1^2} &amp; \frac{\partial^2 \varepsilon_1}{\partial \theta_1 \partial \theta_2} \\ \frac{\partial^2 \varepsilon_1}{\partial \theta_2 \partial \theta_1} &amp; \frac{\partial^2 \varepsilon_1}{\partial \theta_2^2} \end{pmatrix} , \begin{pmatrix} \frac{\partial^2 \varepsilon_2}{\partial \theta_1^2} &amp; \frac{\partial^2 \varepsilon_2}{\partial \theta_1 \partial \theta_2} \\ \frac{\partial^2 \varepsilon_2}{\partial \theta_2 \partial \theta_1} &amp; \frac{\partial^2 \varepsilon_2}{\partial \theta_2^2} \end{pmatrix} = \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 2 \end{pmatrix} , \begin{pmatrix} -t^2 \sin(\theta_1 t) &amp; 0 \\ 0 &amp; 0 \end{pmatrix}\] <p>Suppose that there exists a value \(\theta_\circ \in \mathcal{D}_\mathscr{M}\) that gives a correct description of the system, so that \(\{\varepsilon(t, \theta)\}\) is a sequence of independent random vectors each of zero mean. This implies that \(\varepsilon(t, \theta_\circ)\) is independent of \(z^{t-1}\) and hence of</p> \[\frac{d}{d\theta_i}\frac{d}{d\theta_j}\varepsilon(t,\theta) = \frac{d}{d\theta_i}\frac{d}{d\theta_j} [y(t)-g_\mathscr{M}(\theta;t,z^{t-1})]\\ =-\frac{d}{d\theta_i}\frac{d}{d\theta_j}g_\mathscr{M}(\theta;t,z^{t-1}).\] <p>At the true minimum \(\theta_\circ\),</p> \[\mathbb{E} \left \{ \left [ \frac{d^2}{d\theta^2} \varepsilon^T(t,\theta) \right ] \Lambda^{-1} \varepsilon(t,\theta) \right \}=0\] <p>Then a suitable approximation of Heesion is</p> \[\frac{d^2}{d\theta^2} V(\theta) = \mathbb{E}\psi(t, \theta)\Lambda^{-1}\psi^T(t, \theta)\] <p>This approximation is good when close to the minimum where a true Hessian is more important than elsewhere. With an approximation of the Hessian, the algorithms are often referred to the Gauss-Newton direction.</p> <p>A natural approximation of Hession at \(\theta=\hat{\theta}(t-1)\), based on the observation \(z^{t}\) is then obtained by</p> \[\begin{align} R(t)=\frac{1}{t} \sum_{k=1}^t \psi(k, \hat{\theta}(t-1))\Lambda^{-1}\psi^T(k, \hat{\theta}(t-1)). \label{eq:rt} \end{align}\] <p>This is not recursive because \(\psi(k, \hat{\theta}(t-1))\) can not be computed recursively. Then we have to use</p> \[\begin{align} R(t)=\frac{1}{t} \sum_{k=1}^t \psi(k)\Lambda^{-1}\psi^T(k). \label{eq:app_rt} \end{align}\] <p>where \(\psi(k)\) are determined as … However, since the first terms of sum in \(\eqref{eq:app_rt}\) are computed for parameter estimates far from \(\hat{\theta}(t-1)\), \(\eqref{eq:app_rt}\) is usually not a good approximation of \(\eqref{eq:rt}\). It is better to use a weighted mean where more weight is put on the last values</p> \[\begin{align} R(t)=\frac{1}{t} \sum_{k=1}^t \beta(t,k)\psi(k)\Lambda^{-1}\psi^T(k)+\delta(t)R_0 \label{eq:app_rr} \end{align}\] <p>where \(\delta(t)+\sum_{k=1}^t \beta(t,k)=1\). Also, we can added some prior infomation in \(R_0\). We can also write this in a recursive way</p> \[R(t)=R(t-1)+\alpha(t)[\psi(t)\Lambda^{-1}\psi^T(t) - R(t-1)], R(0)=R_0.\] <p>When \(\alpha(t)\) is chosen larger than \(1/t\), we put more weight on recent measurements.</p> <h3 id="the-choice-of-weighting-matrix">The choice of Weighting Matrix</h3> <p>When the system has scalar output, then a constant \(\Lambda\) acts only as scaling factor. For a multioutput system, the choice of \(\Lambda\) will affect the accuracy of the estimates. The optimal choice is the covariance matrix of the true prediction errors</p> \[\Lambda_\circ = \mathbb{E}[\varepsilon(t,\theta_\circ)\varepsilon^{T}(t,\theta_\circ) ]\] <p>which gives the smallest covariance matrix of the parameter estimates in the offline case. Since \(\Lambda_\circ\) is usually ubnknown, a reasonable choice of \(\Lambda\),</p> \[\hat{\Lambda}(t) = \hat{\Lambda}(t-1) + \alpha(t)[\varepsilon(t)\varepsilon^T(t) - \hat{\Lambda}(t-1)].\] <h3 id="projection-into-the-stability-region">Projection into the Stability Region</h3> <p>The model \(\mathscr{M}\) is obtained as \(\theta\) ranges over the set \(\mathcal{D}_\mathscr{M}\). The generation of prediction is stable only for \(\theta\in \mathcal{D}_s\) where</p> \[\begin{align} D_{s}=\{ \theta \vert \mathscr{F}(\theta) \text{ has all eigenvalues strictly inside the unit circle} \}. \label{eq:Ds} \end{align}\] <p>In fact, in the derivation of the algorithm, we used an assumption that \(\hat{\theta}(t)\in D_{s}\) to adjust \(\xi\) and \(\varepsilon\) in \(\eqref{eq:recursive_xi}\) and \(\eqref{eq:recursive_epsilon}\). This can be accomplished by a project facility of the type</p> \[\hat{\theta}(t) = \left [\hat{\theta}(t-1) + \alpha(t) R^{-1}(t) \psi(t) \Lambda^{-1} \varepsilon(t)\right ]_{D_\mathscr{M}},\] <p>where</p> \[\begin{align} \left [x \right ] _{D_\mathscr{M}}= \left \{\begin{array}{lr} x &amp; \text{if }x \in {D_\mathscr{M}}\\ \text{a value strctily interior to }{D_\mathscr{M}} \text{ if } x \notin {D_\mathscr{M}} \end{array} \right. \end{align}\] <h3 id="summary-of-the-algorithm">Summary of the Algorithm</h3> <p>Now we can summarize the general algorihtm for minizing the quadratic criterion.</p> \[\begin{align} &amp;\varepsilon(t) = y(t)-\hat{y}(t) \\ &amp;\hat{\Lambda}(t) = \hat{\Lambda}(t-1) + \alpha(t)[\varepsilon(t)\varepsilon^T(t) - \hat{\Lambda}(t-1)]\\ &amp;R(t)=R(t-1)+\alpha(t)[\psi(t)\Lambda^{-1}\psi^T(t) - R(t-1)]\\ &amp;\hat{\theta}(t) = \left [\hat{\theta}(t-1) + \alpha(t) R^{-1}(t) \psi(t) \Lambda^{-1} \varepsilon(t)\right ]_{D_\mathscr{M}}\\ &amp;\xi (t+1)= A(\hat{\theta}(t))\xi(t) + B(\hat{\theta}(t))z(t)\\ &amp;\begin{pmatrix} \hat{y}(t) \\ \text{col } \psi(t) \end{pmatrix} = C(\hat{\theta}(t-1)) \xi(t). \label{eq:algo_summary_quad} \end{align}\] <h3 id="equivalent-rerrangement">Equivalent Rerrangement</h3> <p>Introduce</p> \[P(t)=\alpha(t)R^{-1}(t).\] <p>Then,</p> \[P(t)=\frac{1}{\gamma(t)}\left\{ P(t-1)-P(t-1)\psi(t)[\psi^T(t)P(t-1)\psi(t)+\gamma(t)\hat{\Lambda}(t) ]^{-1}\psi^T(t)P(t-1), \right \}\] <p>where</p> \[\gamma(t)=\alpha(t-1)[1-\alpha(t-1)]/\alpha(t-1).\] <p>Using this expression we can write</p> \[\begin{align} L(t)&amp;\triangleq \alpha(t)R^{-1}(t)\psi(t)\hat{\Lambda}^{-1}(t)\\ &amp;=P(t)\psi(t)\hat{\Lambda}^{-1}(t)\\ &amp;=P(t-1)\psi(t)[\psi^T(t)P(t-1)\psi(t)+\gamma(t)\hat{\Lambda}(t) ]^{-1}. \end{align}\] <p>Hence the algorithm can be rewriiten as</p> \[\begin{align} &amp;\varepsilon(t) = y(t)-\hat{y}(t) \\ &amp;\hat{\Lambda}(t) = \hat{\Lambda}(t-1) + \alpha(t)[\varepsilon(t)\varepsilon^T(t) - \hat{\Lambda}(t-1)]\\ &amp;S(t)= \psi^T(t)P(t-1)\psi(t)+\gamma(t)\hat{\Lambda}(t)\\ &amp;L(t)=P(t-1)\psi(t)S^{-1}(t)\\ &amp;\hat{\theta}(t) = \left [\hat{\theta}(t-1) + \alpha(t) R^{-1}(t) \psi(t) \Lambda^{-1} \varepsilon(t)\right ]_{D_\mathscr{M}}\\ &amp;P(t)=[P(t-1)-L(t)S(t)L^T(t)]/\gamma(t)\\ &amp;\xi (t+1)= A(\hat{\theta}(t))\xi(t) + B(\hat{\theta}(t))z(t)\\ &amp;\begin{pmatrix} \hat{y}(t) \\ \text{col } \psi(t) \end{pmatrix} = C(\hat{\theta}(t-1)) \xi(t). \label{eq:algo_summary_quad2} \end{align}\] </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Offline-Identification/">Offline Identification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Theory-Preview/">Preview of Asymptotic Properties of Recursive Identification Methods</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification/">A general framework of Recursive System Identification (Part I)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/User-Summary/">The users' summary of a general recursive identification method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Theory-Heuristic-Discussion/">Convergence Analysis of Recursive Sysid - A Heuristic Discussion</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>