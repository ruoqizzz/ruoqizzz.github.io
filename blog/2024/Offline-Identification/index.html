<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Offline Identification | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2024/Offline-Identification/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Offline Identification</h1> <p class="post-meta">June 19, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/linear-model"> <i class="fas fa-hashtag fa-sm"></i> linear_model</a>     ·   <a href="/blog/category/control"> <i class="fas fa-tag fa-sm"></i> control</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="aspects-of-offline-identification">Aspects of Offline Identification</h2> <p>Offline identification is to identify a model of a system using data that has been collected and stored beforehand. For simplicity, we define \(z^{t}=[y(t)^T, u(t)^T]^T\) as the data at the time step \(t\) and \(\{y_t\}, \{u_t\}\) are output and input signals. Offline identification is a wide subject. Here we only try to point out some aspects that have useful implications for recursive identification</p> <h3 id="identification-as-criterion-minimization">Identification as Criterion Minimization</h3> <p>Given a prediction model,</p> \[\hat{y}(t\vert \theta)=g_\mathscr{M}(\theta;t,z^{t-1})\] <p>a natural measure of its validity is the prediction error,</p> \[\varepsilon(t,\theta)=y(t)-\hat{y}(t\vert \theta)\] <p>Since \(\varepsilon(t,\theta)\) is a \(p\)-dimensional vector, it is useful to introduce a scalar measure</p> \[l(t,\theta,\varepsilon(t,\theta))\] <p>where \(l(\cdot,\cdot,\cdot)\) is a function from \(\mathbb{R}\times \mathbb{R}^d \times \mathbb{R}^p\) to \(\mathbb{R}\). With stored data up to time \(t\), a natural criterion of the validity of the model \(\mathscr{M}(\theta)\) is</p> \[V_N(\theta, z^{N})=\frac{1}{N}\sum_{t=1}^N l (t,\theta, \varepsilon(t,\theta)).\] <p>The offline estimate, denoted by \(\hat{\theta}_N\) is obtained by minimization of \(V_N(\theta, z^{N})\) over \(\theta \in \mathcal{D}_\mathscr{M}\).</p> <h3 id="choices-of-criterion">Choices of Criterion</h3> <h4 id="a-quadratic-criterion">A Quadratic Criterion</h4> <p>Quadratic criterion is a natural way of measuring the size of the prediction error,</p> \[\begin{align} l(t,\theta,\varepsilon) = \frac{1}{2}\varepsilon^T\Lambda^{-1}\varepsilon \label{eq:quad_criterion} \end{align}\] <p>where \(\Lambda\) is a positive definite matrix. A possible disadvantage is that it gives substantial penalties for large errors which might lead to sensitivity to outliers. One simple and common way in system identification is to filter all data before processing.</p> <h4 id="the-maximum-likelihood-criterion">The Maximum Likelihood Criterion</h4> <p>Here we assume that \(\varepsilon(t,\theta)\) is a sequence of independent random vectors and its probability density function is \(\bar{f}(t, \theta, x)\) such that</p> \[P(\varepsilon(t,\theta)\in \mathcal{B}) = \int_\mathcal{B} \bar{f}(t, \theta, x) dx.\] <p>The output at time \(t\) can be written as</p> \[y(t) = g_\mathscr{M}(\theta;t,z^{t-1}) + \varepsilon (t,\theta).\] <p>Under the assumption, we can write the conditional probability density function of \(y(t)\) given \(z^{t-1}\) as</p> \[f(\theta; x_t \vert z^{t-1})=P\left ( y(t)=x_t \vert z^{t-1}, \theta \right) = \bar{f}(t, \theta, y_t-g_\mathscr{M}(\theta;t,z^{t-1})).\] <p>Using Bayes’s rule, the joint density function of \(y(t)\) and \(y(t-1)\) given \(z^{t-2}\) can be expressed as</p> \[\begin{align} f(\theta; x_t, x_{t-1} \vert z^{t-2})&amp;=P\left ( y(t)=x_t, y(t-1)=x_{t-1} \vert z^{t-2}, \theta \right) \\ &amp;= P\left ( y(t)=x_t \vert y(t-1)=x_{t-1} ,z^{t-2}, \theta \right) \cdot P\left ( y(t-1)=x_{t-1} \vert z^{t-2}, \theta \right) \\ &amp;= P\left ( y(t)=x_t \vert z^{t-1}, \theta \right) \cdot P\left ( y(t-1)=x_{t-1} \vert z^{t-2}, \theta \right) \\ &amp;= \bar{f}(t, \theta, y_t-g_\mathscr{M}(\theta;t,z^{t-1})) \cdot \bar{f}(t-1, \theta, y_{t-1}-g_\mathscr{M}(\theta;t-1,z^{t-2})) . \end{align}\] <p>Here we assume that \(\{u^t\}\) is a deterministic sequence. Iterating the foregoing expression from \(t=N\) to \(t=1\) gives the joint probability density of \(y(N), y(N-1),\dots,y(1): f(\theta; x_t, x_{t-1},\dots,x_1)\). By replacing the dummy variables \(xc_i\) with corresponding \(y(i)\), we can obtain</p> \[\log (\theta;y(N), y(N-1),\dots,y(1)) = \sum_{t=1}^N \log \bar{f}(t,\theta,\varepsilon(t,\theta)).\] <p>With \(l(t,\theta,\varepsilon)=-\log\bar{f}(t,\theta,\varepsilon)\), \(\hat{\theta}_N\) equals the maximum likelihood estimate (MLE).</p> <p>If we assume that \(\varepsilon\) has a Gaussian distribution with zero mean and covariance matrix \(\Lambda_t(\theta)\) the</p> \[\begin{align} l(t,\theta,\varepsilon)&amp;=-\log f(t,\theta, \varepsilon)\\ &amp;=\frac{p}{2}\log2\pi + \frac{1}{2}\log \det \Lambda_t(\theta)+\frac{1}{2}\varepsilon^T\Lambda_t^{-1}\varepsilon. \label{eq:loss_log} \end{align}\] <p>Here if \(\Lambda_t\) is known and independent of \(\theta\), the we obtain the quadratic criterion as in \(\eqref{eq:quad_criterion}\).</p> <p><strong>[Relationship of MLE and MAP]</strong></p> <p>MLE and Bayesian maximum a posteriori estimate (MAP) are closely related. Using Bayes’s rule,</p> \[P(\theta \vert y^N)=\frac{P(y^N \vert \theta )P(\theta)}{P(y^N)}\] <p>where \(P(y^N \vert \theta )\) is the likelihood function and \(P(\theta)\) is the prior distribution while \(P(y^N)\) is independent of \(\theta\). Thus, the MAP estimate differs from the MLE estimate only via the prior distribution.</p> <h3 id="asymptotic-properties-of-the-offline-estimate">Asymptotic Properties of the Offline Estimate</h3> <p>Some of the results without proof are from <a href="https://ieeexplore.ieee.org/abstract/document/1101840/" rel="external nofollow noopener" target="_blank">Ljung (1978c)</a> and <a href="https://ieeexplore.ieee.org/abstract/document/4046253/" rel="external nofollow noopener" target="_blank">Ljung and Caines (1979)</a>.</p> <p>Suppose that the limit</p> \[\bar{V}(\theta)=\lim_{N\rightarrow \infty}\mathbb{E} V_N(\theta,z^N)\] <p>exists, where \(\mathbb{E}\) is the expectation operator with respect to \(z^N\), The function \(\bar{V}(\theta)\) thus is the expected value of the criterion for a certain fixed \(\theta\). Then, under weak regularity conditions,</p> \[\hat{\theta}_N \text{ converges w.p. to } \theta^* \text{ a minimum of } \bar{V}(\theta)\] <p>as \(N\) tends to infinity. Notice that this is true whether or not the model set \(\mathscr{M}\) contains the true model. Moreover, if \(\hat{\theta}_N\) converges to \(\theta^*\), such that the matrix \(d^2\bar{V}(\theta^*)/d\theta^2\) is incertible, then</p> \[\sqrt{N}(\hat{\theta}_N -\theta^*)\rightarrow \mathcal{N}(0,P),\] <p>where</p> \[\begin{align} P=[\bar{V}^{''}(\theta^*)]^{-1} \left \{ \lim N \cdot \mathbb{E}[\bar{V}^{}(\theta^*, z^N)]^T \bar{V}^{'}(\theta^*, z^N) \right \} [\bar{V}^{''}(\theta^*)]^{-1} \label{eq:app_P} \end{align}\] <p>where \('\) and \(''\) denotes differentiation once and twice, respectively with respect to \(\theta\).</p> <h3 id="relation-to-cramér-rao-bound">Relation to Cramér-Rao Bound</h3> <p>Suppose there is a value \(\theta_\circ\) in the model set such that with \(\theta=\theta_\circ\) gives a correct description of the true data. Then it can be shown that \(\theta^*=\theta_\circ\) and that the matrix P in \(\eqref{eq:app_P}\) equals the Cramer-Rao lower bound provided that \(l\) is chosen as in \(\eqref{eq:loss_log}\).</p> <h4 id="cremer-rao-bound-and-properties-of-estimators">[Cremer-Rao Bound and properties of estimators]</h4> <p>Consider a random vector in \(\mathbb{R}^n\), \(y^n = \begin{pmatrix} y(1) &amp; y(2) &amp;\dots &amp;y(n) \end{pmatrix}\). Let its joint density function be</p> \[f(\theta; x_1,\dots,x_n) = f(\theta;x^n), \label{eq:f_density}\] <p>which is known up to a finite-dimensional parameter vector \(\theta\) with dimension \(d\). The problem we face is to find an estimate of \(\theta\) based on a sample of \(y^n\).<br> There are certain properties of estimators are desirable. We shall drop the supercrip \(n\) in \(y^n\) when not essential.</p> <p><strong>[Unbiased]</strong></p> <p>An estimator is thus said to be <em>unbiasd</em> if</p> \[\mathbb{E}_\theta \hat{\theta}(y^n) = \theta\] <p>i.e., if its expected value is the true parameter value. For an unbiased estimator, it is of interest to know its variance or covariance around the mean</p> \[\begin{align} P_s=\mathbb{E}[\hat{\theta}_s(y)-\theta][\hat{\theta}_s(y)-\theta]^T. \label{eq:cov} \end{align}\] <p>An estimator \(\hat{\theta}_1(y)\) is said to be more efficient than an estimator \(\hat{\theta}_2(y)\) if</p> \[P_1 \leq P_2.\] <p>where \(\leq\) is the matrix inequality.</p> <p><strong>[Consistent]</strong></p> <p>The estimator is said to be <em>consistent</em> if</p> \[\begin{align} \hat{\theta}(y^n) \rightarrow \theta \text{ as } n \rightarrow \infty. \label{eq:consistent} \end{align}\] <p>Since \(\hat{\theta}(y^n)\) is a random variable, we must specify in what sense this holds. Thus, if \(\eqref{eq:consistent}\) hold w.p.1, it is <em>strongly consistent</em>.</p> <p><strong>[Asumpototic distribution]</strong></p> <p>Central limit theorem can often be applied to \(\hat{\theta}(y^n)\) to infer that \(\sqrt{N}(\hat{\theta}_N -\theta)\) is asymptotically normal with zero mean as \(n\rightarrow \infty\).</p> <p><strong>[The Cramér-Rao Inequality]</strong></p> <p>Naturally, we hope that the estimators are as efficient as possible, ie. ones that make the covariance matrix \(\eqref{eq:cov}\) as small as possible. There is a theoretical lower limit called Cramér-Rao inequality.</p> <p><strong>[Theorem 1]</strong> Suppose that \(y(i)\) may take values in interval, whose limits do not depend on \(\theta\). Suppose that \(\theta\) is a real scalar and that \(f(\theta;\cdot)\) in \(\eqref{eq:f_density}\) is twice continuously differentiable with respect to \(\theta\). Let \(\hat{\theta}(y^n)\) be an estimator of \(\theta\) with expected value \(\mathbb{E}_\theta \hat{\theta}(y^n)=\gamma(\theta)\), which is assumed to be differentiable with respect to \(\theta\). Then</p> \[\mathbb{E}_\theta [\hat{\theta}(y^n)-\gamma(\theta)]^2 \leq -\frac{[d\gamma(\theta)/d\theta^2]}{\mathbb{E}_\theta \partial^2 \log f(\theta;\gamma^n)/\partial \theta^2}.\] <p><em>Proof</em> By definition \(\mathbb{E}_\theta \hat{\theta}(y^n) = \int \hat{\theta}(x^n)f(\theta;x^n)dx^n=\gamma(\theta).\)</p> <p>Differentiate w.r.t \(\theta\):</p> \[\int \hat{\theta}(x^n)\frac{\partial}{\partial \theta}f(\theta;x^n)dx^n \\ = \int \hat{\theta}(x^n) \left \{ \frac{\partial}{\partial \theta}\log f(\theta;x^n)dx^n f(\theta;x^n)dx^n \right \}\\ =\mathbb{E}_\theta \hat{\theta}(y^n) \frac{\partial}{\partial \theta}\log f(\theta;y^n)\\ =\frac{d}{d\theta}\gamma(\theta).\] <p>Since</p> \[\int f(\theta;x^n)dx^n =1,\] <p>we have</p> \[\begin{align} \int \frac{\partial}{\partial \theta}f(\theta;x^n)dx^n = 0\\ \int \frac{\partial}{\partial \theta}\log f(\theta;x^n)\cdot f(\theta;x^n) dx^n = 0 \label{eq:first_dif} \end{align}\] <p>or</p> \[\mathbb{E}_\theta \frac{\partial}{\partial \theta}\log f(\theta; y^n)=0\] <p>and</p> \[\mathbb{E}_\theta \gamma(\theta) \frac{\partial}{\partial \theta}\log f(\theta; y^n)\\ = \gamma(\theta) \mathbb{E}_\theta\frac{\partial}{\partial \theta}\log f(\theta; y^n) \\ =0\] <p>Note that \(\gamma(\theta)\) is a constant with respect to the random variable \(y^n\).<br> Substracting this from previous expression gives</p> <p>\(\mathbb{E}_\theta [\hat{\theta}(y^n) - \gamma(\theta)] \frac{\partial}{\partial \theta}\log f(\theta; y^n) = \frac{d}{d\theta}\gamma(\theta).\) The Schwartz inequality now gives</p> \[\begin{align} \left [ \frac{d}{d\theta}\gamma(\theta) \right]^2 \leq \mathbb{E}_\theta [\hat{\theta}(y^n) - \gamma(\theta)]^2 \cdot \mathbb{E}_\theta \left [\frac{\partial}{\partial \theta}\log f(\theta; y^n)\right]^2. \\ \left(\mathbb{E}_\theta[XY])^2 \leq \mathbb{E}_\theta[X]^2 \mathbb{E}_\theta[Y]^2 \right) \label{eq:proof1} \end{align}\] <p>Also by differentiating \(\eqref{eq:first_dif}\),</p> \[\begin{align} \int \left \{ \frac{\partial^2}{\partial \theta^2}\log f(\theta;x^n) + \left [ \frac{\partial^2}{\partial \theta^2}\log f(\theta;x^n) \right]^2 \right \} f(\theta;x^n) = 0. \\ \left[ \frac{\partial^2}{\partial \theta^2}\log f(\theta;x^n) \right]^2 \Rightarrow -\frac{\partial^2}{\partial \theta^2}\log f(\theta;x^n) \label{eq:proof2} \end{align}\] <p>Combing \(\eqref{eq:proof1}\) and \(\eqref{eq:proof2}\), we get</p> \[\mathbb{E}_\theta [\hat{\theta}(y^n)-\gamma(\theta)]^2 \leq -\frac{[d\gamma(\theta)/d\theta^2]}{\mathbb{E}_\theta \partial^2 \log f(\theta;\gamma^n)/\partial \theta^2}\] <p><strong>[Corllary]</strong> Let \(\hat{\theta}(y^n)\) be a vector-valued unbiased estimator of \(\theta\). Then \(\mathbb{E} [\hat{\theta}(y^n)-\theta] [\hat{\theta}(y^n)-\theta]^T \geq M^{-1}\), where \(M = \mathbb{E}_\theta \left[ \frac{\partial}{\partial \theta}\log f(\theta; y^n) \right ] \left[ \frac{\partial}{\partial \theta}\log f(\theta; y^n) \right ]^T \\ =\mathbb{E}_\theta \frac{\partial^2}{\partial \theta^2} \log f(\theta; y^n)\) is the expected value of the second derivative matrix (the Hessian) of the function \(-\log f\), also known as the fisher information matrix.</p> <h3 id="optimal-choice-of-weights-in-quadratic-criteria">Optimal Choice of Weights in Quadratic Criteria</h3> <p>Now we want to know the optimal choice of the weighting matrix in \(l(t,\theta,\varepsilon) = \frac{1}{2}\varepsilon^T\Lambda^{-1}\varepsilon.\) To do so, we make the assumption that \(\theta^*=\theta_\circ\) where \(\{\varepsilon(t, \theta_\circ)\}\) is a sequence of indenpendet random vectors with zero means and covariance matrices \(\Lambda_\circ\).</p> <p>With asymptotic convariance matrix \(\eqref{eq:app_P}\), we find here</p> <p>\(\begin{align} P = \left [ \mathbb{E} \psi(t, \theta_\circ) \Lambda^{-1} \psi(t, \theta_\circ) \right]^{-1} \mathbb{E} \psi(t, \theta_\circ) \Lambda^{-1} \Lambda_\circ \Lambda^{-1} \psi(t, \theta_\circ) \times \left [ \mathbb{E} \psi(t, \theta_\circ) \Lambda^{-1} \psi(t, \theta_\circ) \right]^{-1}. \label{eq:P_lambda} \end{align}\) where \(\left [ \frac{d}{d\theta} \hat{y}(t\vert \theta \right )] = \psi (t, \theta)\). \(P\) can be seen as a function of \(\Lambda\) and the minimal value of \(P\) is obtained for \(\Lambda=\Lambda_\circ\). Then, we also get best estimates \(\hat{\theta}_N\) in the sense that they are closet to \(\theta_\circ\) according to \(\eqref{eq:P_lambda}\).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification-2/">A general framework of Recursive System Identification (Part II)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Theory-Preview/">Preview of Asymptotic Properties of Recursive Identification Methods</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Convergence-Analysis-of-Recursive-Sysid-with-Associated-Differential-Equation/">Convergence Analysis of Recursive Sysid with Associated Differential Equation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/2024-09-03-Convergence-Analysis-of-Recursive-Sysid-with-Associated-Differential-Equation-General-Case/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-model-based-RL/">Model-based Reinforcement Learning</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>