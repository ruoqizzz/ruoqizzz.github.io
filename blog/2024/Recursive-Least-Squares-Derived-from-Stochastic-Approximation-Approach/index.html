<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Recursive Least Squares Derived from Stochastic Approximation Approach | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2024/Recursive-Least-Squares-Derived-from-Stochastic-Approximation-Approach/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Recursive Least Squares Derived from Stochastic Approximation Approach</h1> <p class="post-meta">June 18, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/linear-model"> <i class="fas fa-hashtag fa-sm"></i> linear_model,</a>   <a href="/blog/tag/recursive-sysid"> <i class="fas fa-hashtag fa-sm"></i> recursive_sysid</a>     ·   <a href="/blog/category/control"> <i class="fas fa-tag fa-sm"></i> control</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="problem-statement">Problem Statement:</h2> <p>Let’s consider the simple difference equation model</p> \[y(t)=\theta^T\phi(t)+v(t)\] <p>where \(y(t)\) and \(\phi(t)\) are measured quantities, \(\theta\) is a determined model parameter and \(v(t)\) is the equation error. It is natural to select \(\theta\) by minimizing the variance of \(v(t)\),</p> \[\begin{align} \min_\theta V(\theta) \label{eq:minimizingV} \end{align}\] <p>where</p> \[V(\theta)=\frac{1}{2}\mathbb{E}\left [y(t)-\phi^T(t)\theta \right]^2\] <p>Since \(V(\theta)\) is quadratic in \(\theta\), \(\eqref{eq:minimizingV}\) can be found by solving</p> \[\left [ -\frac{d}{d\theta}V(\theta) \right ]^T = \mathbb{E}\phi(t)\left [y(t)-\phi^T(t)\theta \right]=0.\] <p>Since the probability distribution of \(y(t), \phi(t)\) is not known in general, one way to replace expectations with sample means which goes to least squares (check Recursive Least Square blog).</p> <h2 id="stochastic-approximation">Stochastic Approximation</h2> <p>Stochastic approximation is a mathematical technique used to find the roots of functions or to optimize functions when there is noise in measurements. Let \(\{e(t)\}\) be a sequence of random variables from the same distribution and \(t\) is the discrete-time variable. A typical problem for stochastic approximation can be to find the solution to</p> \[\begin{align} \mathbb{E}_e Q(x, e(t))=f(x)=0 \label{eq:stochastic_approx_problem} \end{align}\] <p>where in general, the distribution of \(e(t)\) is unknown, the exact form of the function \(Q\) is not unknown but the values can be accessed for any \(x\). In our case,</p> \[\begin{align} x&amp;=\theta\\ e(t)&amp;=\begin{bmatrix}y(t) \\ \phi(t)\end{bmatrix}\\ Q(x, e(t))&amp;=\phi(t)[y(t)-\phi^T(t)\theta], \end{align}\] <p>where \(e(t)\) can be observed and \(Q\) is a known function.</p> <p>One trivial way would be first to fix \(x\), get a huge number of observations $Q(x, e(t))$ for this \(x\) which gives a good estimate of \(f(x)\) and change the value of \(x\) and repeat this procedure. It is more efficient to change the value of \(x\) for each observation to not waste lots of effort on estimating \(f(x)\) where \(x\) values are far away from the solution. It is suggested to use the Robbins-Monro scheme,</p> \[\begin{align} \hat{x}(t)=\hat{x}(t-1) + \alpha(t)Q(\hat{x}(t-1), e(t)), \label{eq:robbins-monro} \end{align}\] <p>where \(\{\alpha(t)\}\) is a sequence of positive scalars tending to zeros (step sizes).</p> <h2 id="apply-to-linear-regression">Apply to Linear Regression</h2> <p>With this, we can get</p> \[\hat{\theta}(t)=\hat{\theta}(t-1)+\alpha(t)\phi(t)\left [y(t)-\phi^T(t)\theta \right].\] <p>The sequence \(\{\alpha(t)\}\) in control literature is called “gain sequence”. Some common choices are</p> <ol> <li>Constant: \(\alpha(t)=\alpha_0\)</li> <li>Normalized: \(\alpha(t)=\alpha_0 / \vert \phi(t)\vert^2\)</li> <li>Normalized and decreasing: \(\alpha(t)=\left[ \sum_{k=1}^t \vert \phi(t)\vert^2\right]^{-1}\)</li> </ol> <p>The normalized choices give the invariance under the scaling of the signals \(y(t)\) and \(\phi(t)\).</p> <h3 id="the-robbins-monro-as-stochastic-gradient-method">The Robbins-Monro as Stochastic Gradient Method</h3> <p>In terms of the general formulation \(\eqref{eq:stochastic_approx_problem}\), we could think the original problem \(\eqref{eq:minimizingV}\) as</p> \[\begin{align} \min_x V(x) \\ V(x)=\mathbb{E}_e J(x, e(t)). \end{align}\] <p>Let</p> \[-\frac{d}{dx}V(x)=f^T(x),\] <p>and suppose that the gradient</p> \[-\frac{d}{dx}J(x, e(t)) = Q^T(x, e(t))\] <p>can be obtained for any chosen \(x\). Then the solution can be obtained by</p> \[0=\left [ -\frac{d}{dx}V(x) \right ]^T =f(x)=\mathbb{E} Q(x, e(t)).\] <p>Now we interchanged expectations and differentiation and back to \(\eqref{eq:stochastic_approx_problem}\). Thus, the Robbins-Monro \(\eqref{eq:robbins-monro}\) can be seen as an algorithm to minimize \(V(x)\) and it adjusts \(x\) in the direction of the negative gradient of the observed $J(x, e(t))$. On average, the adjustments are in the negative gradient direction of \(V(x)\) which is the stochastic gradient method (SGD).</p> <h3 id="newton-direction">Newton Direction</h3> <p>It is well-known that the SGD is fairly inefficient especially when its iterates are getting close to the minimum. The Newtown method can give better results in which the search direction is modified from negative gradient to</p> \[\left [ -\frac{d^2}{dx^2}V(x) \right ]^{-1}\left [ -\frac{d}{dx}V(x) \right ]^T\] <p>where \(\left [ -\frac{d^2}{dx^2}V(x) \right ]\) is the Hessian. The iteration can be written as</p> \[x^{(t+1)} = x^{(t)} - \left [ -\frac{d^2}{dx^2}V(x) \right ]^{-1}\left [ -\frac{d}{dx}V(x) \right ]^T \bigg \vert _{x=x^{(t)}}\] <p>where the iteration will converge in one step to the minimum of \(V(x)\) if its function is quadratic in \(x\). Therefore, it is very efficient when close to the minimum where the approximation of \(V(x)\) well describes the function while otherwise, it is inefficient or even diverges. Thus, the Hessian is usally replaced by a guaranteed positive-definite approximation to secure a search direction that points downhill.</p> <h3 id="a-general-stochastic-newton-method">A General Stochastic Newton Method</h3> <p>Since the hessian gives a clear improvement in effiency, it is reasonable to consider the Newton variant of Robbins-Monro scheme,</p> \[\begin{align} \hat{x}(t)=\hat{x}(t-1) + \alpha(t) \left [ \bar{V}^{''}(\hat{x}(t-1), e(t)) \right]^{-1} Q(\hat{x}(t-1), e(t)), \label{eq:stochastic_newton} \end{align}\] <p>which is called “stochastic Newton algorithm”.</p> <h3 id="apply-to-linear-regression-1">Apply to Linear Regression</h3> <p>For the quadratic criterion \(V(\theta)\), we got</p> \[\frac{d^2}{d\theta^2}V(\theta)=\mathbb{E}_e \phi(t)\phi^T(t),\] <p>which is independent of \(\theta\). The Hessian can be determined as the solution \(R\) of the equation,</p> \[\mathbb{E}[ \phi(t)\phi^T(t) - R] =0.\] <p>Applying the Robbins-Monro scheme,</p> \[R(t) = R(t-1) + \alpha(t)[\phi(t)\phi^T(t)-R(t-1)].\] <p>With this Hession estimate, we can obtain</p> \[\hat{x}(t)=\hat{x}(t-1) + \alpha(t) R^{-1} \phi(t)\left [y(t)-\phi^T(t)\theta(t-1) \right].\] <p>When \(\alpha(t)=\frac{1}{t}\), this coincides with Recursive Least Squares.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification-2/">A general framework of Recursive System Identification (Part II)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Offline-Identification/">Offline Identification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-model-based-RL/">Model-based Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/linear-model/">Recursive Least Squares Derived from Offline Estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Theory-Preview/">Preview of Asymptotic Properties of Recursive Identification Methods</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>