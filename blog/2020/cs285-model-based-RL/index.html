<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Model-based Reinforcement Learning | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2020/cs285-model-based-RL/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Model-based Reinforcement Learning</h1> <p class="post-meta">October 27, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fas fa-tag fa-sm"></i> rl</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Notes of course <a href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="external nofollow noopener" target="_blank">CS285</a></p> <h1 id="model-based-reinforcement-learning">Model-based Reinforcement Learning</h1> <h3 id="rl-objective">RL objective</h3> \[\begin{align} p_\theta(\tau) &amp;= p_\theta(s_1,a_1,\dots,s_T,a_t)=p(s_1)\prod_{t=1}^T \pi_\theta(a_t\vert s_t)p(s_{t+1}\vert s_t,a_t)\\ \theta^* &amp;= \arg\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)}[\sum_t r(s_t,a_t)] \end{align}\] <p>In previous lectures, the policy gradient methods obtain the policy by firstly calculating the gradient of this objective and then doing the gradient descent. Value-based methods obtain the policy by choosing the best action to maximze the expectation of every state. Actor-critic mthods combine those two, introducing the value function to reduce the variance.</p> <p>These model-free methods ignore the dynamic of system/env $p(s_{t+1}\vert s_t, a_t)$, supposing it is not avaible and never try to learn it. In lecture 10~12, model-based methods are discussed from two aspect:</p> 1. Planning based on the known dynamics 2. Methods to learn dynamics and policies <h2 id="planning">Planning</h2> <h3 id="open-loop-and-closed-loop-planning">Open-loop and closed-loop planning</h3> <p>As shown below, the agent with closed-loop planning interact with environment every step and output the action at every time step while that with open-loop planning only get the intial state of environment at the first step and then output the sequence of actions based on the model. The objective of open-loop planning is given by, \(a_1,\dots, a_T = \arg \max_{a_1,\dots,a_T}\sum_{t}r(s_t,a_t)~~s.t. s_{t+1}=f(s_t,a_t)\) where $f(\cdot)$ is the dynamic model of the environment.</p> <p><img src="images/planning.jpg" alt="Open-loop-closed-loop-planning"></p> <p>From the view of model-based RL, open-loop is more reasonable. If the model is given, then there is no need to interate with environment and the agent can decide which action to take based on the inference. However, the dynamics of model is not perfect in practice and open-loop might cause large accumulated error. Closed-loop planning can fix this erro by the reward at each timestep.</p> <h3 id="stochastic-optimization-methods">Stochastic Optimization Methods</h3> <p>Stochastic optimization methods are one kind of black-box optimzation methods, only optimizing the objective with unknown form $J$, \(a_1,\dots, a_T = \arg \max_{a_1,\dots, a_T } = J(a_1,\dots, a_T ).\)</p> <ul> <li> <p>Random shooting method</p> <p>The most intuitive method is to sample several sequences of actions from action distribution, calculate the reward and choose the action sequence with maximum accumulated rewards.</p> <p><img src="images/random-sampling-shooting.png" alt="random shooting method"></p> <p>Random shooting method based on the sampling and more samples lead to better performance.</p> </li> <li> <p>Cross-entropy method</p> <p>Cross-entropy method improve the random shooting method by the way to sample. It model the sample distribution and fit it with the “best” samples.</p> <p><img src="images/CEM.jpg" alt="CEM"></p> <p>In step three, it select <em>elites</em> and fit $p(A)$ by them to increase the proabability near them. Gaussians are usually used in CEM and in this situation, $p(A)$ is similar to fit the distribution of rewards.</p> <p><img src="images/CEM-gaussian.png" alt="CEM Gaussian"></p> </li> </ul> <p>Stochastic optimization methods are simple to implement and fast to run but they are not suitable for tasks with high-dimentional action space.</p> <h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3> <p>The key point of MCTS is to treat <strong>discrete</strong> planning problem as tree search problem. In every layer of tree, it choose the action. Ideally, we can explore every leaves to get the optimal solution. But as the number of layers icrease, the number of leaves us growing exponentially. For high-dimension action space, it is almost impossible to iterate all trajectories.</p> <h4 id="upper-confidence-bounds-for-trees">Upper Confidence bounds for Trees</h4> <p>UCT is a score function, defined by \(Score(s_t) = \frac{Q(s_t)}{N(s_t)} + 2C\sqrt{\frac{2\ln N(s_{t-1})}{N(s_t)}}\) where $Q$ is the acculated value of state $s_t$, $N(s_t)$ is the number of visiting and $C$ is a certain scala. The first term represents the average value and second term represents the balance of visiting which prefers the unvisited states. UCT can be used to choose which node to explore.</p> <h4 id="behavior-policy-and-tree-policy">Behavior Policy and Tree Policy</h4> <p>Behavior policy is how to judge the value of node $Q$, it could be random policy. Tree policy is the policy to choose the node to explore, for example, the UCT policy.</p> <p><img src="images/MCTS.jpg" alt="MCTS"></p> <h3 id="trajectory-optimization">Trajectory Optimization</h3> <h4 id="shooting-methods-and-collocation-methods">Shooting methods and collocation methods</h4> <p>Shooting methods only optimize the actions space of some objectives without constraints, defined by \(min_{u_1,\dots,u_T} c(x_1,u_1)+ c(f(x_1,u_1), u_2)+\dots+c(f(f(\dots)\dots), u_T)\) where $c(\cdot)$ is the cost function and $f(\cdot)$ represents the dynamics. In shooting methods, dynamics only offer the results of planning. If dynamic model of environment is known, we got a optimization problem without constaints. In this situation, there is no constaints on states, thus it is easy to get trajectories with large differences starting from same initial state $s_0$. Also, the first action has enormous influence than the last action, which is difficult to solve.</p> <p><img src="images/shooting.jpg" alt="shooting"></p> <p>Collocation methods optimize over actions and states, with constraints, \(min_{u_1,\dots,u_T,x_1,\dots,x_T} \sum_{t=1}^T c(x_t, u_t)~~\text{s.t. }x_t=f(x_{t-1}, u_{t=1})\) This adding more constraint so the oscillation is smaller.</p> <p><img src="images/collocation.jpg" alt="collocation"></p> <h4 id="linear-quadratic-regulator">Linear Quadratic Regulator</h4> <ul> <li> <p>Problem Statement:</p> <ul> <li>Linear: the model is locally linear and time-varied</li> </ul> \[f(x_t, u_t) = F_t \begin{bmatrix} x_t \\ u_t\end{bmatrix} + f_t\] <ul> <li>Quadratic: cost function is quadraction function</li> </ul> \[c(x_t, u_t) = \frac{1}{2}\begin{bmatrix} x_t \\ u_t\end{bmatrix}^TC_t\begin{bmatrix} x_t \\ u_t\end{bmatrix} + \begin{bmatrix} x_t \\ u_t\end{bmatrix}^T c_t\] </li> <li> <p>Algorithm</p> </li> </ul> <p><img src="images/LQR.jpg" alt="LQR"></p> <h4 id="iterative-lqr">Iterative LQR</h4> <p>In LQR, we supposed the model is locally linear. However, in practice, the model is often non-linear. In iterative LQR, we use the idea of Newton’s method. In Newton’s methods, the algorithm use both the first-order and second-order derivative to find the next guess point. (Gradient descent use only first-order derivate) as shown in graph below.</p> <p><img src="images/newton.jpg" alt="newton-raph"></p> <p><img src="images/newton-algo.jpg" alt="newton-raph"></p> <p>Interative LQR firstly use first-order derivative to approximate the dynamic model, then use second-order derivate to approximate the cost function. \(\begin{align} f(x_t,u_u) &amp;\approx f(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}f(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} \\ c(x_t,u_u) &amp;\approx c(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}c(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} + \frac{1}{2}\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix}^T \nabla_{x_t,u_t}^2 c(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} \end{align}\) Same as \(\begin{align} \bar{f}(\delta x_t, \delta u_t) &amp;= F_t\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix} \\ \bar{f}(\delta x_t, \delta u_t) &amp;=\frac{1}{2}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^T C_t\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix} + \begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^Tc_t\\ \end{align}\) where $ \delta x =x_t - \hat{x_t}, \delta u =u_t - \hat{u_t}$. This form is similar to the form in LQR, so we can use the LQR to get the state and action.</p> <p><img src="images/iLQR.jpg" alt="iLQR"></p> <p>In the forward pass, we still use the nonlinear dynmiacs NOT approximation to get the next state.</p> <p>Paper: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization</p> <h4 id="differential-dynamic-programming">Differential Dynamic Programming</h4> <p>In iLQR, the dynamic model is approximated only using first-order derivative but not second-order. Second-order is used in differential DP, \(f(x_t,u_u) \approx f(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}f(\hat{x_t},\hat{u_t})\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}+ \frac{1}{2}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^T \nabla^2_{x_t,u_t}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}\) <img src="images/DifferentialDP.jpg" alt="Differential-DP"></p> <h2 id="learning-dynamics-and-policies">Learning Dynamics and Policies</h2> <h3 id="naive-model-and-replan">Naive model and Replan</h3> <p>If we know the dynamics model of environment, we can use the dynamics and use the planning methods to make actions.</p> <p>The <strong>naive</strong> model (Mode-based RL 0.5):</p> <blockquote> <ol> <li>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</li> <li>learn dynamics model $f(s,a)$ to minimize $\sum_i \Vert f(s_i,a_i)-s’_i\Vert ^2 $</li> <li>plan through $f(s,a)$ to choose actions</li> </ol> </blockquote> <p>This model works when with fewer parameters in $f(\cdot)$ and good enough base policy $\pi_0$. However, in general it doesnot work because of distribution mismatch, that is $p_{\pi_0}(s_t) \neq p_{\pi_f}(s_t)$. The base policy only explore some region of state space but not the whole one.</p> <p>We can recollect data using the planning actions (Mode-based RL 1.0):</p> <blockquote> <ol> <li>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</li> <li>plan through $f(s,a)$ to choose actions</li> <li>execute those actions and add the resulting data ${(s,a,s’)_j}$ to $\mathcal{D}$</li> <li>Repeat 2-5</li> </ol> </blockquote> <p>This algorithm still has a problem that, the planning actions may cause to large accumulated errors (low quality samples).</p> <p><strong>Replan algorithm (Mode-based RL 1.5):</strong></p> <blockquote> <ol> <li> <p>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</p> </li> <li> <p>learn dynamics model $f(s,a)$ to minimize $\sum_i \Vert f(s_i,a_i)-s’_i\Vert ^2 $</p> </li> <li> <p>plan through $f(s,a)$ to choose actions [<strong>REPLAN</strong>]</p> <ul> <li> <p>execute the first action, observe reulting $s’$</p> </li> <li> <p>append $(s,a,s’)$ to $\mathcal{D}$</p> </li> <li> <p>repeat</p> </li> </ul> </li> <li> <p>repeat 2-3</p> </li> </ol> </blockquote> <p>MPC can bed used to replan the action using short horizon. The more Replan is done, the smaller the perfect degree requirements for model and single planning. In many cases, even random sampling can achieve good results.</p> <h3 id="uncentainty-in-model-based-rl">Uncentainty in model-based RL</h3> <h4 id="performance-gap">Performance gap</h4> <p>Replan algorithm theoratically works well but in pratice, it easily converge to local minima.</p> <p><img src="images/performance-gap.jpg" alt="MBRL-gap"></p> <p>As shown in the graph, the green line is the MBRL Replan 1.5’s performance. It can receive bigger than zero reward but stuck in the local minima while the model-free methods(blue line) based on the model achive much better performance. It might because the planning step in algorithm is overoptimistic by maximizing the reward. For example, in the graph shown below, the plan estimate the reward with the model still have error, then maximzing the reward wrongly.</p> <p><img src="images/plan-overfit.png" alt="MBRL-gap"></p> <p>This problem can be solved by introducing unvertainty. In the planning step, using mean reward instead of max reward can eliminate over-optimistic.</p> <h4 id="uncertainty-aware-rl">Uncertainty-Aware RL</h4> <h4 id="two-types-of-uncertainty">Two types of uncertainty</h4> <ol> <li> <strong>Aleatoric or statistical uncertainty</strong>: describe the uncertainty caused by statistical indicators. For example, if learned model has large uncertainty if the data with big noise.</li> <li> <strong>Epistemic or model uncertainty</strong>: the confidence that model believe its prediction, e.g., GP use the number of data in some region to calculate.</li> </ol> <h4 id="output-entorpy">Output entorpy</h4> <p>Idea 1 is use the output entropy, e.g. from neural network. But this is the statistical uncertainty not model uncertainty.</p> <h4 id="estimate-model-uncertainty">Estimate model uncertainty</h4> <blockquote> <p>the model is certain about the data, but we are not certain about the model $\theta$</p> </blockquote> <p>Usually, we estimate $\theta$ by \(\theta = \arg \max_\theta log p(\theta \vert \mathcal{D}) = \arg \max_\theta log p\mathcal{D} \vert \theta)\) But actually, we can instead estimate $p(\theta \vert \mathcal{D})$ and the entorpy of this model give the model uncetainty. Then, the prediction woudl be \(\int p(s_{t+1}\vert s_t, a_t, \theta)p(\theta \vert \mathcal{D}) d\theta\)</p> <ul> <li> <p>Bayesian nerural network</p> <p><img src="images/bayesian.jpg" alt="bayesian-NN"></p> <p>As shown in the left of graph, the weights of regular nerual network is some real number while in Bayesian NN, the weights are distribution</p> <p><strong>Common approximation</strong>: \(p(\theta \vert \mathcal{D}) = \prod_i p(\theta_i \vert D) p(\theta_i \vert \mathcal{D}) = \mathcal{N}(\mu_i, \sigma_i)\)</p> </li> <li> <p>Bootstrap ensembles</p> <p><img src="images/ensembles.jpg" alt="ensembles"></p> </li> </ul> <p>Model uncertainty can be described by mean of p</p> \[p(\theta \vert \mathcal{D}) \approx \frac{1}{N} \sum_i \delta(\theta_i)\] \[\int p(s_{t+1} \vert s_t,a_t,\theta)p(\theta \vert D) d\theta \approx \frac{1}{N} \sum_i p(s_{t+1} \vert s_t,a_t, \theta_i)\] <p>Note: we are average the probabilities not means.</p> <p>How to train? Bootstrap.</p> <h4 id="planning-with-uncertainty">Planning with uncertainty</h4> <p>With $N$ models, the objective change to \(J(a_1,\dots,a_H) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^H r(s_{t,i}, a_t), \text{s.t. } s_{t+1,i} = f(s_{t,i}, a_t)\) The algorithm can be</p> <blockquote> <p>In general, for candidate action sequence $a_1, \dots, a_H$:</p> <p>​ Step 1: sample $\theta \sim p(\theta \vert \mathcal{D})$ (sample a model!)</p> <p>​ Step 2: at eatch time step $t$, sample $s_{t+1} \sim p(s_{t+1} \vert s_t, a_t, \theta)$</p> <p>​ Step 3: calculate $R = \sum_t r(s_t,a_t)$</p> <p>​ Step 4: repeat steps 1 to 3 and accumulate the acerage reward</p> </blockquote> <p>Firstly, from data samples model parameter $\theta$ and get transition. Repeat several times, get the mean of accumulated reward.</p> <h3 id="latent-space-model-learn-model-from-images">Latent space model (Learn Model from Images)</h3> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Offline-Identification/">Offline Identification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification-2/">A general framework of Recursive System Identification (Part II)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-Model-free-RL/">Model-free Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Convergence-Analysis-of-Recursive-Sysid-with-Associated-Differential-Equation/">Convergence Analysis of Recursive Sysid with Associated Differential Equation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Recursive-Least-Squares-Derived-from-Stochastic-Approximation-Approach/">Recursive Least Squares Derived from Stochastic Approximation Approach</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>