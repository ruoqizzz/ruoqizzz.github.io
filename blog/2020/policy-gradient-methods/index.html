<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Policy Gradient Methods | Ruoqi Zhang</title> <meta name="author" content="Ruoqi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%88&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruoqizzz.github.io/blog/2020/policy-gradient-methods/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruoqi </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talk</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Policy Gradient Methods</h1> <p class="post-meta">September 11, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/algo"> <i class="fas fa-hashtag fa-sm"></i> algo</a>     ·   <a href="/blog/category/rl"> <i class="fas fa-tag fa-sm"></i> rl</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Policy gradient methods mentioned:</p> <ol> <li>REINFORCE</li> <li>Off-Policy Actor Critic</li> <li>DPG</li> <li>DDPG</li> <li>TD3</li> </ol> <p>Policy: \(\pi_\theta (a\vert s) =\text{Pr}\{A_t=a\vert S_t=s\}\)</p> <p>Performance measure:</p> \[\begin{align*} J(\theta) &amp;= \sum_{s\in S}d^\pi(s)V^\pi(s)=\sum_{s\in S}d^\pi(s)\sum_{a\in A}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) \\ d^\pi(s) &amp;= \text{lim}_{t \rightarrow \infty } P(s_t = s| s_0, \pi_0) \end{align*}\] <p>For simplicity, the parameter \(\theta\) would be ommitted for policy \(\pi_\theta\).</p> <p>Update: \(\theta_{t+1} = \theta_{t} + \alpha \widehat{\nabla J(\theta_t)}\)</p> <h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2> \[\begin{align*} \nabla_{\theta} J(\theta)&amp; = \nabla_{\theta} \Big (\sum_{s\in S}d^\pi(s)V^\pi(s) \Big)\\ &amp; =\sum_{s\in S}d^\pi(s) \nabla_{\theta} V^\pi(s) \\ \end{align*}\] \[\begin{align*} \nabla_{\theta} V^\pi(s) &amp;= \nabla_{\theta}[\sum_{a\in A}\pi_{\theta}(a\vert s)Q^{\pi}(s,a)] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \nabla_{\theta} Q^{\pi}(s,a) ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \nabla_{\theta} \sum_{s',r}P(s',r|s,a)(r + V^{\pi}(s')) ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s',r}P(s',r|s,a)\nabla_{\theta} V^{\pi}(s') ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s'}P(s'|s,a)\nabla_{\theta} V^{\pi}(s') ] &amp; \scriptstyle{\text{; here } P(s' | s, a) = \sum_r P(s', r | s, a)} \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s'}P(s'|s,a) \\ &amp;~~~~~~~~~~~ \sum_{a'\in A} [\nabla_{\theta}\pi_{\theta}(a'|s')Q^{\pi}(s',a') + \pi_{\theta}(a\vert s) \sum_{s''}P(s''|s',a')\nabla_{\theta} V^{\pi}(s'') ] \end{align*}\] <p>Recursion</p> \[\begin{align*} \nabla_{\theta} V^\pi(s) &amp;= \sum_{x\in S}\sum_{k=0}^{\infty} \text{Pr}(s \rightarrow x, k, \pi)\sum_{a\in A} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \end{align*}\] <p>Simplify the notation by assming that every episode starts in some particular state \(s_0\) (not random).</p> <p>Then, \(J(\theta) \doteq v_{\pi_\theta}(s_0)\)</p> <p>Therefore,</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;= \nabla v_\pi(s_0) \\ &amp;= \sum_{s}\Big( \sum_{k=0}^{\infty} \text{Pr}(s_0 \rightarrow s, k, \pi)\Big) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s} \eta(s)\sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;= \sum_{s'} \eta(s') \frac{\sum_{s} \eta(s)}{\sum_{s'} \eta(s')} \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;= \sum_{s'} \eta(s') \sum_{s}\frac{ \eta(s)}{\sum_{s'} \eta(s')} \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s'} \eta(s') \sum_{s} d^{\pi}(s)\sum_{a}\nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;\propto\sum_{s\in S}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \end{align*}\] <p>Sometime, written as,</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;\propto\sum_{s\in S}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s\in S}d^\pi(s)\sum_{a} \pi_\theta(a\vert s)Q^\pi(s,a) \frac{\nabla_\theta\pi_\theta(a\vert s)}{\pi_\theta(a\vert s)} \\ &amp;= \mathbb{E}_\pi[Q^\pi(s,a)\nabla_\theta\ln\pi_\theta(a\vert s)] \end{align*}\] <p>Note: This vailla policy gradient upsate has no bias but high variance.</p> <h2 id="methods">Methods</h2> <h3 id="reinforce-monte-carlo-policy-gradient">REINFORCE (Monte-Carlo policy gradient)</h3> \[\begin{align*} \nabla_\theta J(\theta) &amp;\propto\sum_{s}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \mathbb{E}_\pi \sum_{a} [\nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)] \\ &amp;= \mathbb{E}_\pi[\sum_{a} \pi_\theta(a\vert S_t) Q^\pi(S_t,a) \frac{\nabla_\theta\pi_\theta(a\vert S_t)}{\pi_\theta(a\vert S_t)}] &amp; \scriptstyle{;~S_t\text{ is the sample state}} \\ &amp;= \mathbb{E}_\pi[ Q^\pi(S_t,A_t) \frac{\nabla_\theta\pi_\theta(A_t|S_t)}{\pi_\theta(A_t|S_t)}] &amp; \scriptstyle{;~A_t \sim \pi \text{ is the sample action}} \\ &amp;= \mathbb{E}_\pi[ G_t\frac{\nabla_\theta\pi_\theta(A_t|S_t)}{\pi_\theta(A_t|S_t)}] &amp; \scriptstyle{;~G_t \text{ is the return}} \end{align*}\] <h3 id="off-policy-policy-gradient">Off-Policy Policy Gradient</h3> <p>Behavior policy: \(\beta(a\vert s)\)</p> <p>Performance measure: \(J(\theta) = \sum_s d^\beta(s)\sum_a Q^\pi(s,a) \pi_\theta(a\vert s)=\mathbb{E}_{s\sim d^\beta}[\sum_a Q^\pi(s,a) \pi_\theta(a\vert s)]\)</p> \[\begin{align*} \nabla_\theta J(\theta) &amp;= \nabla_\theta \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \Big] &amp; \\ &amp;= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \big( Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) + {\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)} \big) \Big] \\ &amp;\stackrel{(i)}{\approx} \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) \Big] &amp; \scriptstyle{\text{; Ignore the second part} }. \\ &amp;= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} \Big] &amp; \\ &amp;= \mathbb{E}_\beta \Big[\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s) \Big] &amp; \scriptstyle{\text{; The blue part is the importance weight.}} \end{align*}\] <p>\(\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}\) is the importance weight.</p> <h3 id="dpg">DPG</h3> <p>Deterministic policy: \(a=\mu_\theta(s)\)</p> <p>The initial distribution over states: \(\rho_0(s)\)</p> <p>Probability density at step \(k\) following policy \(\mu\): \(\rho^\mu(s\rightarrow s') = \text{Pr}(s \rightarrow s', k, \mu)\)</p> <p>Discounted state distrubution: \(\rho^\mu(s') = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s', k) ds\)</p> <p>Performance measure:</p> \[J(\theta) = \int_\mathcal{S} \rho^\mu(s) Q(s, \mu_\theta(s)) ds\] <p><strong>Deterministic policy gradient theorem</strong></p> <p>Move the policy in the direction of the gradient of \(Q\) rather than globally maximising \(Q\). Specifically, for each visited state \(s\), the policy parameters are updated in proportion to the gradient \(\nabla_\theta Q^{\mu}(s, \mu_\theta(s))\) . Each state suggests a different direction of policy improvement; these may be averaged together by taking an expectation with respect to the state distribution \(\rho^\mu(s')\)</p> \[\begin{aligned} \nabla_\theta J(\theta) &amp;= \int_\mathcal{S} \rho^\mu(s) \nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\ &amp;= \mathbb{E}_{s \sim \rho^\mu} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] &amp; ;\scriptstyle{ \text{chain rule}} \end{aligned}\] <p>\(Q\) is a function of both \(s\) and \(\mu(a\vert s)\) which rely on \(\theta\) and \(s\). So, \(\frac{\partial Q}{\partial \theta} = \frac{\partial \mu}{\partial \theta} \frac{\partial Q}{\partial \mu}\)</p> <p>\(\nabla_\theta \mu_\theta(s)\) Is the Jacobian matrix such that each column is the gradient \(\nabla_\theta [\mu_\theta(s)]_d\) of the \(d\)-th action dimension of the policy with respect to the policy parameters \(\theta\).</p> <p>Example of on-policy actor-critic algorithm (SARSA):</p> \[\begin{aligned} \delta_t &amp;= R_t + \gamma Q^w(s_{t+1}, a_{t+1}) - Q^w(s_t, a_t) &amp; \scriptstyle{\text{; TD error}}\\ w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) &amp; \\ \theta_{t+1} &amp;= \theta_t + \alpha_\theta {\nabla_a Q^w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} \end{aligned}\] <p>where \(w_t\) is the weight of critc estaimating \(Q^w(s,a) \approx Q^\pi(s,a)\) at time step \(t\).</p> <p><strong>Off-policy</strong></p> <p>The training samples are generated by a stochastic policy \(\beta(a\vert s)\):</p> \[\begin{aligned} J_\beta(\theta) &amp;= \int_\mathcal{S} \rho^\beta Q^\mu(s, \mu_\theta(s)) ds \\ \nabla_\theta J_\beta(\theta) &amp;= \mathbb{E}_{s \sim \rho^\beta} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ] \end{aligned}\] <p>Example of off-policy actor-critic algorithm (Q-learning):</p> \[\begin{aligned} \delta_t &amp;= R_t + \gamma Q^w(s_{t+1}, \mu_\theta(s_{t+1})) - Q^w(s_t, a_t) &amp; \scriptstyle{\text{; TD error}}\\ w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) &amp; \\ \theta_{t+1} &amp;= \theta_t + \alpha_\theta {\nabla_a Q^w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} \end{aligned}\] <h3 id="ddpg">DDPG</h3> <p>Main work: combining the DPG with DQN</p> <p><img src="images/ddpg-algo.png" alt="DDPG"></p> <h3 id="td3">TD3</h3> <p>Motivated by Double Q-learning and Double-DQN.</p> <p>Difference from DDPG:</p> <ol> <li> <p>Clipped Double Q-learning:</p> <ul> <li> <p>Two critics \(Q^{w_1}\) and \(Q^{w_2}\) as Double Q-learning and only one actor (actor and actor-target)</p> </li> <li> <p>Use the minimum estimation of two critics</p> \[y=r + \gamma \min_{i=1,2}Q^{w_i}(s', \mu_\theta(s'))\] </li> </ul> </li> <li> <p>Delayed update of Target and Policy Networks</p> </li> <li> <p>Target Policy smoothing</p> \[\begin{align*} y &amp;= r+\gamma Q_w(s', \mu_\theta(s')+\epsilon) \\ \epsilon &amp;\sim \text{clip}(\mathcal{N}(0, \sigma), -c, +c) \end{align*}\] </li> </ol> <p><img src="images/TD3.png" alt="TD3"></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-Model-free-RL/">Model-free Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/cs285-model-based-RL/">Model-based Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification-2/">Offline Identification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Recursive-Least-Squares-Derived-from-Stochastic-Approximation-Approach/">Recursive Least Squares Derived from Stochastic Approximation Approach</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/A-general-framework-of-Recursive-System-Identification/">A general framework of Recursive System Identification (Part I)</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruoqi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>