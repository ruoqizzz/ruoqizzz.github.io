<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ruoqizzz.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ruoqizzz.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-18T11:19:00+00:00</updated><id>https://ruoqizzz.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Linear Models in Recursive System Identification</title><link href="https://ruoqizzz.github.io/blog/2024/linear-model/" rel="alternate" type="text/html" title="Linear Models in Recursive System Identification"/><published>2024-06-15T16:05:00+00:00</published><updated>2024-06-15T16:05:00+00:00</updated><id>https://ruoqizzz.github.io/blog/2024/linear-model</id><content type="html" xml:base="https://ruoqizzz.github.io/blog/2024/linear-model/"><![CDATA[<h2 id="linear-difference-equation">Linear Difference Equation</h2> \[y(t)+a_1y(t-1) + \dots + a_n y(t-n) = b_1u(t-1)+\dots+b_mu(t-m)+v(t) \label{eq:linear}\] <p>where \(\{u(t)\},\{y(t)\}\) are input and output signals and \(v(t)\) is some disturbance of unspecified character.</p> <p>Let \(q^{-1}\) be backward shift operator, then</p> \[A(q^{-1})y(t) = B(q^{-1})u(t) + v(t),\] <p>where \(A(q^{-1})\) and \(B(q^{-1})\) are polynomials in the delay operator:</p> \[A(q^{-1}) = 1+a_1q^{-1}+\dots + a_nq^{-n}\\ B(q^{-1}) = 1+b_1q^{-1}+\dots + b_m q^{-m}\] <p>Introduce the vector of lagged input-output data (regressor),</p> \[\phi^T(t)=\begin{bmatrix} -y(t-1) &amp;\dots -y(t-n)&amp;u(t-1)&amp;\dots u(t-m) \end{bmatrix}^T\] <p>Then, \(~\eqref{eq:linear}\) can be rewritten as</p> \[y(t)=\theta^T\phi(t)+v(t)\] <p>where \(\theta^T=\begin{bmatrix}a_1 &amp; \dots &amp;a_n &amp;b_1 &amp;\dots b_m \end{bmatrix}^T\) is the paramter vector.</p> <p>If the character of the disturbance term \(v(t)\) is not specified, it is natural to use</p> \[\hat{y}(t\vert \theta)\triangleq\theta^T\phi(t)\] <p>as the prediction of \(y(t)\) having observed previous inputs and outputs.</p> <h3 id="offline-identification-the-least-squares">Offline Identification: The least squares</h3> <p>The parameter vector can be estimated from the measurements of \(y(t)\) and \(\phi(t)\) with $$t=1,2,\dots,N$. A common way to choose the estimation is to minimize</p> \[V_N(\theta)=\frac{1}{N}\sum_1^N \alpha_t[y(t)-\theta^T\phi(t)]^2\] <p>with respect to \(\theta\) and \(\{\alpha_t\}\) is a sequence of positive numbers allowing to give different weights to different observations. This criterion \(V_N(\theta)\) is quadratic in \(\theta\) and thus it can be minimized analytically,</p> \[\hat{\theta}(N) = \left[ \sum_{1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \sum_{1}^N \alpha_t\phi(t)y(t), \label{eq:offline_ls}\] <p>where we assume that the inverse exists. It can be written in a recursive fashion. Let</p> \[\bar{R}(t) = \sum_{1}^N \alpha_t\phi(t)\phi(t)^T.\] <p>Then, from \(\eqref{eq:offline_ls}\), we can get</p> \[\sum_{1}^N \alpha_t\phi(t)y(t) = \bar{R}(t-1)\hat{\theta}(t-1).\] <p>From the definition of \(\bar{R}(t)\),</p> \[\bar{R}(t-1)=\bar{R}(t)-\alpha_t\phi(t)\phi^T(t)\] <p>Thus \(\begin{align} \hat{\theta}(t) &amp;=\bar{R}^{-1}(t)\left[ \sum_{k=1}^{t-1} \alpha_k\phi(k)y(k) + \alpha_t \phi(t)y(t) \right]\\ &amp;=\bar{R}^{-1}(t)\left[ \bar{R}(t-1)\hat{\theta}(t-1)+ \alpha_t \phi(t)y(t) \right]\\ &amp;=\bar{R}^{-1}(t)\left[ \bar{R}(t)\hat{\theta}(t-1)+ \alpha_t \phi(t)[-\phi^T(t)\hat{\theta}(t-1) + y(t)] \right]\\ &amp;=\hat{\theta}(t-1) + \bar{R}^{-1}(t)\phi(t)\alpha_t[y(t) - \hat{\theta}^T(t-1)\phi(t)] \end{align}\)</p> <p>and</p> \[\bar{R}(t)=\bar{R}(t-1) + \alpha_t\phi(t)\phi^T(t)\] <p>Sometimes we may prefer to work with</p> \[R(t) \triangleq \frac{1}{t}\bar{R}(t)\] <p>Then</p> \[R(t)=\frac{1}{t} \left [\bar{R}(t-1) + \alpha_t\phi(t)\phi^T(t)\right]=\frac{t-1}{t} R(t-1)+\frac{1}{t}\alpha_t\phi(t)\phi^T(t)\\ =R(t-1) + \frac{1}{t}[ \alpha_t\phi(t)\phi^T(t)-R(t-1) ]\] <p>In summary, we can write \(\begin{align} \hat{\theta}(t)&amp;=\hat{\theta}(t-1) + \frac{1}{t}R^{-1}(t)\phi(t)\alpha_t [y(t)-\theta^T(t-1)\phi(t)],\\ R(t)&amp;=R(t-1) + \frac{1}{t}[ \alpha_t\phi(t)\phi^T(t)-R(t-1)]. \label{eq:offline_rls1} \end{align}\)</p> <h3 id="an-equivalent-form-recursive-least-squares">An Equivalent Form: Recursive Least Squares</h3> <p>Equation \(\eqref{eq:offline_rls1}\) is not that suited for computation since a matrix inverse has to be calculated in each time step. It’s more natural to introduce</p> \[P(t)=\bar{R}^{-1}=\frac{1}{t}R^{-1}(t)\] <p>and update \(P(t)\) directly instead. This can be done by matrix inverse lemma</p> \[[A+BCD]^{-1}=A^{-1}-A^{-1}B[DA^{-1}B+C^{-1}]^{-1}DA^{-1}.\] <p>The proof can be done by multiplying the RHS by $$(A+BCD)$.</p> <p>Let \(A=P(t-1)$,\)B=\phi(t)$, \(C=\alpha_t\) and \(D=\phi^T(t)\),</p> \[\begin{align} P(t)&amp;=\left[P^{-1}(t-1)+\phi(t)\alpha_t\phi^T(t) \right]^{-1}\\ &amp;=P(t-1)-P(t-1)\phi(t) \left[ \phi^T(t)P(t-1)\phi(t)+\frac{1}{\alpha_t} \right]^{-1}\phi^T(t)P(t-1)\\ &amp;=P(t-1)-\frac{P(t-1)\phi(t)\phi^T(t)P(t-1)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)} \end{align}\] <p>Now the inversion of a square matrix of dim \(\theta\) is replaced by inversion of a scalar \(\alpha_t\).</p> <p>Thus the recursive least squares can be written as</p> \[\begin{align} \hat{\theta}(t)&amp;=\hat{\theta}(t-1) + L(t)[y(t) -\hat{\theta}^T(t-1)\phi(t)],\\ L(t)&amp;=\frac{P(t-1)\phi(t)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)},\\ P(t)&amp;=P(t-1)-\frac{P(t-1)\phi(t)\phi^T(t)P(t-1)}{1/\alpha_t + \phi^T(t)P(t-1)\phi(t)}. \label{eq:rls} \end{align}\] <h3 id="initial-conditions">Initial Conditions</h3> <p>The only assumption we made is the \(\bar{R}(t)\) is invertible and typically it becomes invertible at time \(t_0=dim~ \phi(t) = dim~\theta\). Thus strictly speaking, the proper initials values for \(\eqref{eq:rls}\) are obtained if starting at the time \(t_0\) for which</p> \[P(t_0)= \left [ \sum_{k}^{t_0} \alpha_k\phi(k)\phi(k)^T \right ]^{-1} \\ \hat{\theta}(t_0) = P(t_0)\sum_{k}^{t_0}\alpha_k \phi(k)y(k).\] <p>It is more common to start at \(t=0\) with some invertible matrix \(P(0)\) and a vector $$\hat{\theta}(0)$. Then, the resulting estimates are</p> \[\hat{\theta}(t) = \left [ P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\left [ P^{-1}(0)\hat{\theta}(0)+\sum_{k=1}^t \alpha_k\phi(k)y(k) \right].\] <p>We can see that the relative importance of the initial values decays over time as the magnitudes of the sums increase. Also, as \(P^{-1}(0)\rightarrow0\), the recursive estimate goes to the offline one. A common choice of initial values is to take \(P(0)=C \cdot I\) and \(\hat{\theta}(0)=0\), where \(C\) is some large constant.</p> <h3 id="asymptotic-properties">Asymptotic Properties</h3> <p>We assume that the data are generated by</p> \[y(t) = \theta_\circ \phi(t) + v(t)\] <p>Inserting this equation to \(\eqref{eq:offline_ls}\),</p> \[\begin{align} \hat{\theta}(N) &amp;= \left[ \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \left \{ \sum_{t=1}^N \alpha_t\left[\phi(t)\phi^T(t)\theta_\circ+\phi(t)v(t)\right] \right\}\\ &amp;= \theta_\circ + \left[ \frac{1}{N} \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t) \end{align}\] <p>According to the law of large numbers, the sum \(\frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t)\) will converge to its expected values as \(N\) goes to infinity. The expected values depend on the correlation between the disturbance term \(v(t)\) and the data vector \(\phi(t)\). It’s zero only when \(v(t)\) and \(\phi(t)\) are uncorrelated. This is true when</p> <ul> <li>\(\{v(t)\}\) is i.i.d with zero means</li> <li>\(n=0\) and \(\{u(t)\}\) is independent of the zero-mean noise sequence \(\{v(t)\}\)</li> </ul> <p>In both cases, the \(\hat{\theta}(N)\) approaches to \(\theta_\circ\) as \(N\) goes to infinity.</p> <h3 id="interpretations-of-rls">Interpretations of RLS</h3> \[\begin{align} \hat{\theta}(N) &amp;= \left[ \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \left \{ \sum_{t=1}^N \alpha_t\left[\phi(t)\phi^T(t)\theta_\circ+\phi(t)v(t)\right] \right\}\\ &amp;= \theta_\circ + \left[ \frac{1}{N} \sum_{t=1}^N \alpha_t\phi(t)\phi(t)^T \right]^{-1} \frac{1}{N}\sum_{t=1}^N \alpha_t\phi(t)v(t) \end{align}\] <ol> <li> <p>Beforehand, we have shown that how RLS derived from the offline LS version and \(\hat{\theta}(t) = \left [ P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\left [ P^{-1}(0)\hat{\theta}(0)+\sum_{k=1}^t \alpha_k\phi(k)y(k) \right]\)</p> <p>With \(P^{-1}(0)=0\), it minimizes the least sqaures criterion</p> \[V_t(\theta)=\frac{1}{t}\sum_{k=1}^t \alpha_k[y(k)-\theta^T\phi(k)]^2\] </li> <li> <p>The estimate \(\hat{\theta}\) can be seen as the Kalman filter state estimate for state-space model \(\theta(t+1) =\theta(t)\\ y(t)=\phi^T(t)\theta(t)+v(t)\)</p> </li> <li> <p>The RLS is a recursive minimization of \(\bar{V}(\theta)=\mathbb{E}\frac{1}{2}[y(t)-\theta^T\phi(t)]^2.\)</p> <p>The factor \(\phi(t)[y(t) -\hat{\theta}^T(t-1)\phi(t)]\) is then an estimate of the gradient while</p> \[t\cdot P(t) =\left [ \frac{1}{t} P^{-1}(0)+\sum_{k=1}^t \alpha_k\phi(k)\phi^T(k) \right]^{-1}\] <p>is the inverse of an estimate of the second derivate of the criterion. The updating \(\hat{\theta}(t)\) is thus updated with the direction of “Newton” and a decaying step size \(\frac{1}{t}\).</p> </li> </ol>]]></content><author><name></name></author><category term="control"/><category term="linear_model,"/><category term="recursive_sysid"/><summary type="html"><![CDATA[Linear Difference Equation]]></summary></entry><entry><title type="html">Model-free Reinforcement Learning</title><link href="https://ruoqizzz.github.io/blog/2020/cs285-Model-free-RL/" rel="alternate" type="text/html" title="Model-free Reinforcement Learning"/><published>2020-10-27T11:04:00+00:00</published><updated>2020-10-27T11:04:00+00:00</updated><id>https://ruoqizzz.github.io/blog/2020/cs285-Model-free-RL</id><content type="html" xml:base="https://ruoqizzz.github.io/blog/2020/cs285-Model-free-RL/"><![CDATA[<p>Notes of course <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">CS285</a> Lecture 05-08:</p> <ul> <li>Policy Gradient</li> <li>Actor-Critic</li> <li>Value Function</li> <li>Deep RL with Q-functions</li> </ul> <h2 id="policy-gradient">Policy Gradient</h2> <p>The goal of RL:</p> \[\begin{align*} \theta^* &amp;= \arg \max_\theta E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]\\ &amp;= \arg \max_\theta J(\theta) \end{align*}\] <p>A natural thought is to use gradient of \(J(\theta)\) to get the best policy.</p> <h3 id="direct-policy-differentiation">Direct policy differentiation</h3> \[\begin{align*} J(\theta) &amp;= E_{\tau\sim p_\theta(\tau)}[r(\tau)] \\ &amp;= \int p_\theta(\tau)r(\tau)d\tau\\ \nabla_\theta J(\theta)&amp;= \int \nabla_\theta p_\theta(\tau)r(\tau)d\tau \\ &amp;= \int p_\theta(\tau) \nabla_\theta \log p_\theta(\tau)r(\tau)d\tau \\ &amp;= E_{\tau\sim p_\theta(\tau)}[\nabla_\theta \log p_\theta(\tau)r(\tau)d\tau] \\ \nabla_\theta \log p_\theta(\tau) &amp;= \nabla_\theta \big[\log p(s_1) + \sum_{t=1}^T \log \pi_\theta(a_t\vert s_t) + \log p(s_{t+1\vert s_t,a_t})\big]\\ &amp;= \nabla_\theta \big[\sum_{t=1}^T \log \pi_\theta(a_t\vert s_t)\big] \\ \text{Therefore, }\\ \nabla_\theta J(\theta)&amp;=E_{\tau\sim p_\theta(\tau)}\Big[\Big (\sum_{t=1}^T \log \pi_\theta(a_t\vert s_t)\Big) +\Big(\sum_{t=1}^Tr(s_t,a_t) \Big) \Big] \\ &amp;\approx \frac{1}{N} \sum_{i=1}^N \Big(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}\vert s_{i,t}) \Big) \Big(\sum_{t=1}^T r(s_{i,t},a_{i,t})\Big) \end{align*}\] <h3 id="reinforce-algorithm">REINFORCE algorithm:</h3> <ol> <li>Sample {\(\tau^i\)} from \(\pi_\theta(a_t\vert s_t)\) (run the policy)</li> <li>Caculate \(\nabla_\theta J(\theta)\)</li> <li>Update rule:\(\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\)</li> <li>Back to step 1</li> </ol> <h3 id="what-is-wrong-with-this---high-variance">What is wrong with this: HIGH VARIANCE!</h3> <h3 id="reducing-variance">Reducing variance</h3> <ul> <li><strong>Causality</strong>: policy at time \(t'\) can not effect reward at time \(t\) when \(t &lt; t'\)</li> </ul> \[\begin{align*} \nabla_\theta J(\theta) &amp;\approx \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) \Big (\sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) \Big)\\ &amp;= \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) \hat{Q}_{i,t} \end{align*}\] <ul> <li> <p><strong>Baselines</strong></p> \[\begin{align*} \nabla_\theta J(\theta) &amp;\approx \frac{1}{N} \sum_{i=1}^N \nabla_\theta \log p_\theta(\tau) r(\tau)\\ &amp;=\frac{1}{N} \sum_{i=1}^N \nabla_\theta \log p_\theta(\tau) [r(\tau) -b]\\ b&amp;= \frac{1}{N} \sum_{i=1}^N r(\tau) \end{align*}\] <p>Still <strong>unbiased</strong>! Although it is not the best baseline, but it’s pretty good</p> </li> </ul> <h3 id="off-policy-policy-gradients">Off-policy Policy Gradients</h3> <p>What if we don’t have samples from \(p_\theta(\tau)\)?</p> <p>Idea: using <strong>Importance sampling</strong> \(\begin{align*} J(\theta) &amp;= E_{\tau\sim p_\theta(\tau)}[r(\tau)] \\ &amp;= E_{\tau\sim \bar{p}(\tau)}\big[\frac{p_\theta(\tau)}{\bar{p}(\tau)}r(\tau)\big] \\ \frac{p_\theta(\tau)}{\bar{p}(\tau)}&amp;= \frac{p(s_1)\prod_{t=1}^T\pi_\theta(a_t\vert s_t)p(s_{t+1\vert s_t,a_t})}{p(s_1)\prod_{t=1}^T \bar{\pi}(a_t\vert s_t)p(s_{t+1\vert s_t,a_t})} \\ &amp;=\frac{\prod_{t=1}^T\pi_\theta(a_t\vert s_t)}{\prod_{t=1}^T \bar{\pi}(a_t\vert s_t)} \\ \nabla_{\theta'} J(\theta')&amp;=E_{\tau\sim p_\theta(\tau)} \Big[\frac{p_{\theta'}(\tau)}{p_{\theta}(\tau)} \nabla_{\theta'}\log \pi_{\theta'}(\tau)r(\tau) \Big]\\ &amp;=E_{\tau\sim p_\theta(\tau)} \Big[ \Big(\prod_{t=1}^T\frac{\pi_{\theta'}(a_t\vert s_t)}{\pi_\theta(a_t\vert s_t)}\Big) \Big(\sum_{t=1}^T \nabla_{\theta'}\log \pi _{\theta'}(a_t \vert s_t) \Big) \Big(\sum_{t=1}^T r(s_t,a_t)\Big) \Big]&amp;\text{what about causality?}\\ &amp;= E_{\tau\sim p_\theta(\tau)} \Big[ \sum_{t=1}^T \nabla_{\theta'}\log \pi _{\theta'}(a_t \vert s_t) \Big(\prod_{t'=1}^t\frac{\pi_{\theta'}(a_{t'}\vert s_{t'})}{ \pi_\theta(a_{t'}\vert s_{t'})}\Big) \Big(\sum_{t'=t}^T r(s_{t'},a_{t'}) \Big(\prod_{t''=t}^{t'} \frac{\pi_{\theta'}(a_{t''}\vert s_{t''})}{\pi_\theta(a_{t''}\vert s_{t''})}\Big) \Big) \Big] \end{align*}\)</p> <p>·</p> <h2 id="actor-critic-algorithms">Actor-Critic Algorithms</h2> <h3 id="idea">Idea</h3> <p>Can we improve the policy gradient from \(\begin{align*} \nabla_\theta J(\theta) &amp;\approx\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) \Big (\sum_{t'=t}^T r(s_{i,t'},a_{i,t'}) \Big)\\ &amp;= \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) \hat{Q}_{i,t} \end{align*}\) Use <strong>true</strong> expected reward-to-go: \(Q(s_t,a_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'},a_{t'})\vert s_t,a_t]\) Also the <strong>baseline</strong>, use the value of state: \(V(s_t) = E_{a_t\sim \pi_\theta(a_t\vert s_t)}[Q(s_t,a_t)]\) Then the gradient can be rewritten as, \(\begin{align*} \nabla_\theta J(\theta) &amp;\approx \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) [Q(s_{i,t},a_{i,t}) - V(s_{i,t})] \\ &amp;= \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_{i,t}\vert s_{i,t}) A(s_{i,t},a_{i,t}) \\ \end{align*}\) The better we estimate the Advantage function \(A(s,a)\), the lower the variance is.</p> <p>Therefore, we introduce the critic to approximate the value functions.</p> <h3 id="policy-evaluation-monte-carlo-with-function-approximation">Policy evaluation (Monte Carlo with function approximation)</h3> <p>Use supervise learning with training data: \(\{\big(s_{i,t}, \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}) \big)\}\) Valuetion function with paramter \(\phi\): \(\phi = \arg\min_\phi L(\phi) = \arg\min_\phi \frac{1}{2}\sum_i\Vert \hat{V}^\pi_{\phi}(s_i) - y_i \Vert^2\)</p> <p>Improve, use the privious fitted value function instead \(\{\big(s_{i,t}, r(s_{i,t}, a_{i,t})+\hat{V}_\phi^\pi(s_{i,t+1}) \big)\}\)</p> <h3 id="an-actor-critic-algorithm">An actor-critic algorithm</h3> <ol> <li>Sample {\(s_i,a_i\)} from \(\pi_\theta(a_t\vert s_t)\) (run the policy)</li> <li>Fit \(\hat{V}_\phi^\pi\) to sampled reward sums</li> <li>Evaluate \(\hat{A}_\phi^\pi(s_i,a_i) = r(s_i,a_i) +\hat{V}_\phi^\pi(s_i') - \hat{V}_\phi^\pi(s_i)\)</li> <li>Caculate \(\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta (a_{i}\vert s_{i}) A(s_{i},a_{i})\)</li> <li>Update rule:\(\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\)</li> <li>Back to step 1</li> </ol> <h3 id="discount-factors">Discount factors</h3> <p>What if episode length is infinity? \(y_{i,t} \approx r(s_{i,t},a_{i,t}) + \gamma\hat{V}_\phi^\pi(s_{i,t})\) The gamma can be considered as, the agent has the probability \(1-\gamma\) to die.</p> <h3 id="an-actor-critic-algorithm-with-discount">An actor-critic algorithm with discount</h3> <p><strong>Batch actor-critic</strong>:</p> <ol> <li>Sample {\(s_i,a_i\)} from \(\pi_\theta(a_t\vert s_t)\) (run the policy)</li> <li>Fit \(\hat{V}_\phi^\pi\) to sampled reward sums</li> <li>Evaluate \(\hat{A}_\phi^\pi(s_i,a_i) = r(s_i,a_i) + \gamma \hat{V}_\phi^\pi(s_i') - \hat{V}_\phi^\pi(s_i)\)</li> <li>Caculate \(\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta (a_{i}\vert s_{i}) A(s_{i},a_{i})\)</li> <li>Update rule:\(\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\)</li> <li>Back to step 1</li> </ol> <p><strong>Online actor-critic:</strong></p> <ol> <li>Take action \(a \sim \pi_\theta(a_t\vert s_t)\), get \((s,a,s',r')\)</li> <li>Update \(\hat{V}_\phi^\pi\) using target \(r + \gamma \hat{V}_\phi^\pi(s_i')\)</li> <li>Evaluate \(\hat{A}_\phi^\pi(s_i,a_i) = r(s_i,a_i) + \gamma \hat{V}_\phi^\pi(s_i') - \hat{V}_\phi^\pi(s_i)\)</li> <li>Caculate \(\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta (a_{i}\vert s_{i}) A(s_{i},a_{i})\)</li> <li>Update rule:\(\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\)</li> <li>Back to step 1</li> </ol> <h3 id="other-imrpovement-methods">Other imrpovement methods</h3> <ul> <li>Q-prop</li> <li>Eligibility traces &amp; n-step returns</li> <li>Generlized advantage estimation</li> </ul> <h2 id="value-function-methods">Value Function Methods</h2> <h2 id="deep-rl-with-q-functions">Deep RL with Q-Functions</h2>]]></content><author><name></name></author><category term="rl"/><summary type="html"><![CDATA[Notes of course CS285 Lecture 05-08:]]></summary></entry><entry><title type="html">Model-based Reinforcement Learning</title><link href="https://ruoqizzz.github.io/blog/2020/cs285-model-based-RL/" rel="alternate" type="text/html" title="Model-based Reinforcement Learning"/><published>2020-10-27T11:04:00+00:00</published><updated>2020-10-27T11:04:00+00:00</updated><id>https://ruoqizzz.github.io/blog/2020/cs285-model-based-RL</id><content type="html" xml:base="https://ruoqizzz.github.io/blog/2020/cs285-model-based-RL/"><![CDATA[<p>Notes of course <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">CS285</a></p> <p>[TOC]</p> <h1 id="model-based-reinforcement-learning">Model-based Reinforcement Learning</h1> <h3 id="rl-objective">RL objective</h3> \[\begin{align} p_\theta(\tau) &amp;= p_\theta(s_1,a_1,\dots,s_T,a_t)=p(s_1)\prod_{t=1}^T \pi_\theta(a_t\vert s_t)p(s_{t+1}\vert s_t,a_t)\\ \theta^* &amp;= \arg\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)}[\sum_t r(s_t,a_t)] \end{align}\] <p>In previous lectures, the policy gradient methods obtain the policy by firstly calculating the gradient of this objective and then doing the gradient descent. Value-based methods obtain the policy by choosing the best action to maximze the expectation of every state. Actor-critic mthods combine those two, introducing the value function to reduce the variance.</p> <p>These model-free methods ignore the dynamic of system/env $p(s_{t+1}\vert s_t, a_t)$, supposing it is not avaible and never try to learn it. In lecture 10~12, model-based methods are discussed from two aspect:</p> 1. Planning based on the known dynamics 2. Methods to learn dynamics and policies <h2 id="planning">Planning</h2> <h3 id="open-loop-and-closed-loop-planning">Open-loop and closed-loop planning</h3> <p>As shown below, the agent with closed-loop planning interact with environment every step and output the action at every time step while that with open-loop planning only get the intial state of environment at the first step and then output the sequence of actions based on the model. The objective of open-loop planning is given by, \(a_1,\dots, a_T = \arg \max_{a_1,\dots,a_T}\sum_{t}r(s_t,a_t)~~s.t. s_{t+1}=f(s_t,a_t)\) where $f(\cdot)$ is the dynamic model of the environment.</p> <p><img src="images/planning.jpg" alt="Open-loop-closed-loop-planning"/></p> <p>From the view of model-based RL, open-loop is more reasonable. If the model is given, then there is no need to interate with environment and the agent can decide which action to take based on the inference. However, the dynamics of model is not perfect in practice and open-loop might cause large accumulated error. Closed-loop planning can fix this erro by the reward at each timestep.</p> <h3 id="stochastic-optimization-methods">Stochastic Optimization Methods</h3> <p>Stochastic optimization methods are one kind of black-box optimzation methods, only optimizing the objective with unknown form $J$, \(a_1,\dots, a_T = \arg \max_{a_1,\dots, a_T } = J(a_1,\dots, a_T ).\)</p> <ul> <li> <p>Random shooting method</p> <p>The most intuitive method is to sample several sequences of actions from action distribution, calculate the reward and choose the action sequence with maximum accumulated rewards.</p> <p><img src="images/random-sampling-shooting.png" alt="random shooting method"/></p> <p>Random shooting method based on the sampling and more samples lead to better performance.</p> </li> <li> <p>Cross-entropy method</p> <p>Cross-entropy method improve the random shooting method by the way to sample. It model the sample distribution and fit it with the “best” samples.</p> <p><img src="images/CEM.jpg" alt="CEM"/></p> <p>In step three, it select <em>elites</em> and fit $p(A)$ by them to increase the proabability near them. Gaussians are usually used in CEM and in this situation, $p(A)$ is similar to fit the distribution of rewards.</p> <p><img src="images/CEM-gaussian.png" alt="CEM Gaussian"/></p> </li> </ul> <p>Stochastic optimization methods are simple to implement and fast to run but they are not suitable for tasks with high-dimentional action space.</p> <h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3> <p>The key point of MCTS is to treat <strong>discrete</strong> planning problem as tree search problem. In every layer of tree, it choose the action. Ideally, we can explore every leaves to get the optimal solution. But as the number of layers icrease, the number of leaves us growing exponentially. For high-dimension action space, it is almost impossible to iterate all trajectories.</p> <h4 id="upper-confidence-bounds-for-trees">Upper Confidence bounds for Trees</h4> <p>UCT is a score function, defined by \(Score(s_t) = \frac{Q(s_t)}{N(s_t)} + 2C\sqrt{\frac{2\ln N(s_{t-1})}{N(s_t)}}\) where $Q$ is the acculated value of state $s_t$, $N(s_t)$ is the number of visiting and $C$ is a certain scala. The first term represents the average value and second term represents the balance of visiting which prefers the unvisited states. UCT can be used to choose which node to explore.</p> <h4 id="behavior-policy-and-tree-policy">Behavior Policy and Tree Policy</h4> <p>Behavior policy is how to judge the value of node $Q$, it could be random policy. Tree policy is the policy to choose the node to explore, for example, the UCT policy.</p> <p><img src="images/MCTS.jpg" alt="MCTS"/></p> <h3 id="trajectory-optimization">Trajectory Optimization</h3> <h4 id="shooting-methods-and-collocation-methods">Shooting methods and collocation methods</h4> <p>Shooting methods only optimize the actions space of some objectives without constraints, defined by \(min_{u_1,\dots,u_T} c(x_1,u_1)+ c(f(x_1,u_1), u_2)+\dots+c(f(f(\dots)\dots), u_T)\) where $c(\cdot)$ is the cost function and $f(\cdot)$ represents the dynamics. In shooting methods, dynamics only offer the results of planning. If dynamic model of environment is known, we got a optimization problem without constaints. In this situation, there is no constaints on states, thus it is easy to get trajectories with large differences starting from same initial state $s_0$. Also, the first action has enormous influence than the last action, which is difficult to solve.</p> <p><img src="images/shooting.jpg" alt="shooting"/></p> <p>Collocation methods optimize over actions and states, with constraints, \(min_{u_1,\dots,u_T,x_1,\dots,x_T} \sum_{t=1}^T c(x_t, u_t)~~\text{s.t. }x_t=f(x_{t-1}, u_{t=1})\) This adding more constraint so the oscillation is smaller.</p> <p><img src="images/collocation.jpg" alt="collocation"/></p> <h4 id="linear-quadratic-regulator">Linear Quadratic Regulator</h4> <ul> <li> <p>Problem Statement:</p> <ul> <li>Linear: the model is locally linear and time-varied</li> </ul> \[f(x_t, u_t) = F_t \begin{bmatrix} x_t \\ u_t\end{bmatrix} + f_t\] <ul> <li>Quadratic: cost function is quadraction function</li> </ul> \[c(x_t, u_t) = \frac{1}{2}\begin{bmatrix} x_t \\ u_t\end{bmatrix}^TC_t\begin{bmatrix} x_t \\ u_t\end{bmatrix} + \begin{bmatrix} x_t \\ u_t\end{bmatrix}^T c_t\] </li> <li> <p>Algorithm</p> </li> </ul> <p><img src="images/LQR.jpg" alt="LQR"/></p> <h4 id="iterative-lqr">Iterative LQR</h4> <p>In LQR, we supposed the model is locally linear. However, in practice, the model is often non-linear. In iterative LQR, we use the idea of Newton’s method. In Newton’s methods, the algorithm use both the first-order and second-order derivative to find the next guess point. (Gradient descent use only first-order derivate) as shown in graph below.</p> <p><img src="images/newton.jpg" alt="newton-raph"/></p> <p><img src="images/newton-algo.jpg" alt="newton-raph"/></p> <p>Interative LQR firstly use first-order derivative to approximate the dynamic model, then use second-order derivate to approximate the cost function. \(\begin{align} f(x_t,u_u) &amp;\approx f(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}f(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} \\ c(x_t,u_u) &amp;\approx c(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}c(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} + \frac{1}{2}\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix}^T \nabla_{x_t,u_t}^2 c(\hat{x_t},\hat{u_t})\begin{bmatrix} x_t - \hat{x_t}\\ u_t - \hat{u_t}\end{bmatrix} \end{align}\) Same as \(\begin{align} \bar{f}(\delta x_t, \delta u_t) &amp;= F_t\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix} \\ \bar{f}(\delta x_t, \delta u_t) &amp;=\frac{1}{2}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^T C_t\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix} + \begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^Tc_t\\ \end{align}\) where $ \delta x =x_t - \hat{x_t}, \delta u =u_t - \hat{u_t}$. This form is similar to the form in LQR, so we can use the LQR to get the state and action.</p> <p><img src="images/iLQR.jpg" alt="iLQR"/></p> <p>In the forward pass, we still use the nonlinear dynmiacs NOT approximation to get the next state.</p> <p>Paper: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization</p> <h4 id="differential-dynamic-programming">Differential Dynamic Programming</h4> <p>In iLQR, the dynamic model is approximated only using first-order derivative but not second-order. Second-order is used in differential DP, \(f(x_t,u_u) \approx f(\hat{x_t},\hat{u_t}) + \nabla_{x_t,u_t}f(\hat{x_t},\hat{u_t})\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}+ \frac{1}{2}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}^T \nabla^2_{x_t,u_t}\begin{bmatrix} \delta x_t \\ \delta u_t\end{bmatrix}\) <img src="images/DifferentialDP.jpg" alt="Differential-DP"/></p> <h2 id="learning-dynamics-and-policies">Learning Dynamics and Policies</h2> <h3 id="naive-model-and-replan">Naive model and Replan</h3> <p>If we know the dynamics model of environment, we can use the dynamics and use the planning methods to make actions.</p> <p>The <strong>naive</strong> model (Mode-based RL 0.5):</p> <blockquote> <ol> <li>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</li> <li>learn dynamics model $f(s,a)$ to minimize $\sum_i \Vert f(s_i,a_i)-s’_i\Vert ^2 $</li> <li>plan through $f(s,a)$ to choose actions</li> </ol> </blockquote> <p>This model works when with fewer parameters in $f(\cdot)$ and good enough base policy $\pi_0$. However, in general it doesnot work because of distribution mismatch, that is $p_{\pi_0}(s_t) \neq p_{\pi_f}(s_t)$. The base policy only explore some region of state space but not the whole one.</p> <p>We can recollect data using the planning actions (Mode-based RL 1.0):</p> <blockquote> <ol> <li>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</li> <li>plan through $f(s,a)$ to choose actions</li> <li>execute those actions and add the resulting data ${(s,a,s’)_j}$ to $\mathcal{D}$</li> <li>Repeat 2-5</li> </ol> </blockquote> <p>This algorithm still has a problem that, the planning actions may cause to large accumulated errors (low quality samples).</p> <p><strong>Replan algorithm (Mode-based RL 1.5):</strong></p> <blockquote> <ol> <li> <p>run base policy $\pi_0(a_t\vert s_t)$ (e.g. random policy) to collect $\mathcal{D}={(s,a,s’)_i}$</p> </li> <li> <p>learn dynamics model $f(s,a)$ to minimize $\sum_i \Vert f(s_i,a_i)-s’_i\Vert ^2 $</p> </li> <li> <p>plan through $f(s,a)$ to choose actions [<strong>REPLAN</strong>]</p> <ul> <li> <p>execute the first action, observe reulting $s’$</p> </li> <li> <p>append $(s,a,s’)$ to $\mathcal{D}$</p> </li> <li> <p>repeat</p> </li> </ul> </li> <li> <p>repeat 2-3</p> </li> </ol> </blockquote> <p>MPC can bed used to replan the action using short horizon. The more Replan is done, the smaller the perfect degree requirements for model and single planning. In many cases, even random sampling can achieve good results.</p> <h3 id="uncentainty-in-model-based-rl">Uncentainty in model-based RL</h3> <h4 id="performance-gap">Performance gap</h4> <p>Replan algorithm theoratically works well but in pratice, it easily converge to local minima.</p> <p><img src="images/performance-gap.jpg" alt="MBRL-gap"/></p> <p>As shown in the graph, the green line is the MBRL Replan 1.5’s performance. It can receive bigger than zero reward but stuck in the local minima while the model-free methods(blue line) based on the model achive much better performance. It might because the planning step in algorithm is overoptimistic by maximizing the reward. For example, in the graph shown below, the plan estimate the reward with the model still have error, then maximzing the reward wrongly.</p> <p><img src="images/plan-overfit.png" alt="MBRL-gap"/></p> <p>This problem can be solved by introducing unvertainty. In the planning step, using mean reward instead of max reward can eliminate over-optimistic.</p> <h4 id="uncertainty-aware-rl">Uncertainty-Aware RL</h4> <h4 id="two-types-of-uncertainty">Two types of uncertainty</h4> <ol> <li><strong>Aleatoric or statistical uncertainty</strong>: describe the uncertainty caused by statistical indicators. For example, if learned model has large uncertainty if the data with big noise.</li> <li><strong>Epistemic or model uncertainty</strong>: the confidence that model believe its prediction, e.g., GP use the number of data in some region to calculate.</li> </ol> <h4 id="output-entorpy">Output entorpy</h4> <p>Idea 1 is use the output entropy, e.g. from neural network. But this is the statistical uncertainty not model uncertainty.</p> <h4 id="estimate-model-uncertainty">Estimate model uncertainty</h4> <blockquote> <p>the model is certain about the data, but we are not certain about the model $\theta$</p> </blockquote> <p>Usually, we estimate $\theta$ by \(\theta = \arg \max_\theta log p(\theta \vert \mathcal{D}) = \arg \max_\theta log p\mathcal{D} \vert \theta)\) But actually, we can instead estimate $p(\theta \vert \mathcal{D})$ and the entorpy of this model give the model uncetainty. Then, the prediction woudl be \(\int p(s_{t+1}\vert s_t, a_t, \theta)p(\theta \vert \mathcal{D}) d\theta\)</p> <ul> <li> <p>Bayesian nerural network</p> <p><img src="images/bayesian.jpg" alt="bayesian-NN"/></p> <p>As shown in the left of graph, the weights of regular nerual network is some real number while in Bayesian NN, the weights are distribution</p> <p><strong>Common approximation</strong>: \(p(\theta \vert \mathcal{D}) = \prod_i p(\theta_i \vert D) p(\theta_i \vert \mathcal{D}) = \mathcal{N}(\mu_i, \sigma_i)\)</p> </li> <li> <p>Bootstrap ensembles</p> <p><img src="images/ensembles.jpg" alt="ensembles"/></p> </li> </ul> <p>Model uncertainty can be described by mean of p</p> \[p(\theta \vert \mathcal{D}) \approx \frac{1}{N} \sum_i \delta(\theta_i)\] \[\int p(s_{t+1} \vert s_t,a_t,\theta)p(\theta \vert D) d\theta \approx \frac{1}{N} \sum_i p(s_{t+1} \vert s_t,a_t, \theta_i)\] <p>Note: we are average the probabilities not means.</p> <p>How to train? Bootstrap.</p> <h4 id="planning-with-uncertainty">Planning with uncertainty</h4> <p>With $N$ models, the objective change to \(J(a_1,\dots,a_H) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^H r(s_{t,i}, a_t), \text{s.t. } s_{t+1,i} = f(s_{t,i}, a_t)\) The algorithm can be</p> <blockquote> <p>In general, for candidate action sequence $a_1, \dots, a_H$:</p> <p>​ Step 1: sample $\theta \sim p(\theta \vert \mathcal{D})$ (sample a model!)</p> <p>​ Step 2: at eatch time step $t$, sample $s_{t+1} \sim p(s_{t+1} \vert s_t, a_t, \theta)$</p> <p>​ Step 3: calculate $R = \sum_t r(s_t,a_t)$</p> <p>​ Step 4: repeat steps 1 to 3 and accumulate the acerage reward</p> </blockquote> <p>Firstly, from data samples model parameter $\theta$ and get transition. Repeat several times, get the mean of accumulated reward.</p> <h3 id="latent-space-model-learn-model-from-images">Latent space model (Learn Model from Images)</h3>]]></content><author><name></name></author><category term="rl"/><summary type="html"><![CDATA[Notes of course CS285]]></summary></entry><entry><title type="html">Policy Gradient Methods</title><link href="https://ruoqizzz.github.io/blog/2020/policy-gradient-methods/" rel="alternate" type="text/html" title="Policy Gradient Methods"/><published>2020-09-11T12:23:00+00:00</published><updated>2020-09-11T12:23:00+00:00</updated><id>https://ruoqizzz.github.io/blog/2020/policy-gradient-methods</id><content type="html" xml:base="https://ruoqizzz.github.io/blog/2020/policy-gradient-methods/"><![CDATA[<p>Policy gradient methods mentioned:</p> <ol> <li>REINFORCE</li> <li>Off-Policy Actor Critic</li> <li>DPG</li> <li>DDPG</li> <li>TD3</li> </ol> <p>Policy: \(\pi_\theta (a\vert s) =\text{Pr}\{A_t=a\vert S_t=s\}\)</p> <p>Performance measure:</p> \[\begin{align*} J(\theta) &amp;= \sum_{s\in S}d^\pi(s)V^\pi(s)=\sum_{s\in S}d^\pi(s)\sum_{a\in A}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) \\ d^\pi(s) &amp;= \text{lim}_{t \rightarrow \infty } P(s_t = s| s_0, \pi_0) \end{align*}\] <p>For simplicity, the parameter \(\theta\) would be ommitted for policy \(\pi_\theta\).</p> <p>Update: \(\theta_{t+1} = \theta_{t} + \alpha \widehat{\nabla J(\theta_t)}\)</p> <h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2> \[\begin{align*} \nabla_{\theta} J(\theta)&amp; = \nabla_{\theta} \Big (\sum_{s\in S}d^\pi(s)V^\pi(s) \Big)\\ &amp; =\sum_{s\in S}d^\pi(s) \nabla_{\theta} V^\pi(s) \\ \end{align*}\] \[\begin{align*} \nabla_{\theta} V^\pi(s) &amp;= \nabla_{\theta}[\sum_{a\in A}\pi_{\theta}(a\vert s)Q^{\pi}(s,a)] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \nabla_{\theta} Q^{\pi}(s,a) ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \nabla_{\theta} \sum_{s',r}P(s',r|s,a)(r + V^{\pi}(s')) ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s',r}P(s',r|s,a)\nabla_{\theta} V^{\pi}(s') ] \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s'}P(s'|s,a)\nabla_{\theta} V^{\pi}(s') ] &amp; \scriptstyle{\text{; here } P(s' | s, a) = \sum_r P(s', r | s, a)} \\ &amp;= \sum_{a\in A} [\nabla_{\theta}\pi_{\theta}(a\vert s)Q^{\pi}(s,a) + \pi_{\theta}(a\vert s) \sum_{s'}P(s'|s,a) \\ &amp;~~~~~~~~~~~ \sum_{a'\in A} [\nabla_{\theta}\pi_{\theta}(a'|s')Q^{\pi}(s',a') + \pi_{\theta}(a\vert s) \sum_{s''}P(s''|s',a')\nabla_{\theta} V^{\pi}(s'') ] \end{align*}\] <p>Recursion</p> \[\begin{align*} \nabla_{\theta} V^\pi(s) &amp;= \sum_{x\in S}\sum_{k=0}^{\infty} \text{Pr}(s \rightarrow x, k, \pi)\sum_{a\in A} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \end{align*}\] <p>Simplify the notation by assming that every episode starts in some particular state \(s_0\) (not random).</p> <p>Then, \(J(\theta) \doteq v_{\pi_\theta}(s_0)\)</p> <p>Therefore,</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;= \nabla v_\pi(s_0) \\ &amp;= \sum_{s}\Big( \sum_{k=0}^{\infty} \text{Pr}(s_0 \rightarrow s, k, \pi)\Big) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s} \eta(s)\sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;= \sum_{s'} \eta(s') \frac{\sum_{s} \eta(s)}{\sum_{s'} \eta(s')} \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;= \sum_{s'} \eta(s') \sum_{s}\frac{ \eta(s)}{\sum_{s'} \eta(s')} \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s'} \eta(s') \sum_{s} d^{\pi}(s)\sum_{a}\nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)\\ &amp;\propto\sum_{s\in S}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \end{align*}\] <p>Sometime, written as,</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;\propto\sum_{s\in S}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \sum_{s\in S}d^\pi(s)\sum_{a} \pi_\theta(a\vert s)Q^\pi(s,a) \frac{\nabla_\theta\pi_\theta(a\vert s)}{\pi_\theta(a\vert s)} \\ &amp;= \mathbb{E}_\pi[Q^\pi(s,a)\nabla_\theta\ln\pi_\theta(a\vert s)] \end{align*}\] <p>Note: This vailla policy gradient upsate has no bias but high variance.</p> <h2 id="methods">Methods</h2> <h3 id="reinforce-monte-carlo-policy-gradient">REINFORCE (Monte-Carlo policy gradient)</h3> \[\begin{align*} \nabla_\theta J(\theta) &amp;\propto\sum_{s}d^\pi(s) \sum_{a} \nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a) \\ &amp;= \mathbb{E}_\pi \sum_{a} [\nabla_\theta\pi_\theta(a\vert s)Q^\pi(s,a)] \\ &amp;= \mathbb{E}_\pi[\sum_{a} \pi_\theta(a\vert S_t) Q^\pi(S_t,a) \frac{\nabla_\theta\pi_\theta(a\vert S_t)}{\pi_\theta(a\vert S_t)}] &amp; \scriptstyle{;~S_t\text{ is the sample state}} \\ &amp;= \mathbb{E}_\pi[ Q^\pi(S_t,A_t) \frac{\nabla_\theta\pi_\theta(A_t|S_t)}{\pi_\theta(A_t|S_t)}] &amp; \scriptstyle{;~A_t \sim \pi \text{ is the sample action}} \\ &amp;= \mathbb{E}_\pi[ G_t\frac{\nabla_\theta\pi_\theta(A_t|S_t)}{\pi_\theta(A_t|S_t)}] &amp; \scriptstyle{;~G_t \text{ is the return}} \end{align*}\] <h3 id="off-policy-policy-gradient">Off-Policy Policy Gradient</h3> <p>Behavior policy: \(\beta(a\vert s)\)</p> <p>Performance measure: \(J(\theta) = \sum_s d^\beta(s)\sum_a Q^\pi(s,a) \pi_\theta(a\vert s)=\mathbb{E}_{s\sim d^\beta}[\sum_a Q^\pi(s,a) \pi_\theta(a\vert s)]\)</p> \[\begin{align*} \nabla_\theta J(\theta) &amp;= \nabla_\theta \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \Big] &amp; \\ &amp;= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \big( Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) + {\pi_\theta(a \vert s) \nabla_\theta Q^\pi(s, a)} \big) \Big] \\ &amp;\stackrel{(i)}{\approx} \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) \Big] &amp; \scriptstyle{\text{; Ignore the second part} }. \\ &amp;= \mathbb{E}_{s \sim d^\beta} \Big[ \sum_{a \in \mathcal{A}} \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} \Big] &amp; \\ &amp;= \mathbb{E}_\beta \Big[\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s) \Big] &amp; \scriptstyle{\text{; The blue part is the importance weight.}} \end{align*}\] <p>\(\frac{\pi_\theta(a \vert s)}{\beta(a \vert s)}\) is the importance weight.</p> <h3 id="dpg">DPG</h3> <p>Deterministic policy: \(a=\mu_\theta(s)\)</p> <p>The initial distribution over states: \(\rho_0(s)\)</p> <p>Probability density at step \(k\) following policy \(\mu\): \(\rho^\mu(s\rightarrow s') = \text{Pr}(s \rightarrow s', k, \mu)\)</p> <p>Discounted state distrubution: \(\rho^\mu(s') = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s', k) ds\)</p> <p>Performance measure:</p> \[J(\theta) = \int_\mathcal{S} \rho^\mu(s) Q(s, \mu_\theta(s)) ds\] <p><strong>Deterministic policy gradient theorem</strong></p> <p>Move the policy in the direction of the gradient of \(Q\) rather than globally maximising \(Q\). Specifically, for each visited state \(s\), the policy parameters are updated in proportion to the gradient \(\nabla_\theta Q^{\mu}(s, \mu_\theta(s))\) . Each state suggests a different direction of policy improvement; these may be averaged together by taking an expectation with respect to the state distribution \(\rho^\mu(s')\)</p> \[\begin{aligned} \nabla_\theta J(\theta) &amp;= \int_\mathcal{S} \rho^\mu(s) \nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\ &amp;= \mathbb{E}_{s \sim \rho^\mu} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] &amp; ;\scriptstyle{ \text{chain rule}} \end{aligned}\] <p>\(Q\) is a function of both \(s\) and \(\mu(a\vert s)\) which rely on \(\theta\) and \(s\). So, \(\frac{\partial Q}{\partial \theta} = \frac{\partial \mu}{\partial \theta} \frac{\partial Q}{\partial \mu}\)</p> <p>\(\nabla_\theta \mu_\theta(s)\) Is the Jacobian matrix such that each column is the gradient \(\nabla_\theta [\mu_\theta(s)]_d\) of the \(d\)-th action dimension of the policy with respect to the policy parameters \(\theta\).</p> <p>Example of on-policy actor-critic algorithm (SARSA):</p> \[\begin{aligned} \delta_t &amp;= R_t + \gamma Q^w(s_{t+1}, a_{t+1}) - Q^w(s_t, a_t) &amp; \scriptstyle{\text{; TD error}}\\ w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) &amp; \\ \theta_{t+1} &amp;= \theta_t + \alpha_\theta {\nabla_a Q^w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} \end{aligned}\] <p>where \(w_t\) is the weight of critc estaimating \(Q^w(s,a) \approx Q^\pi(s,a)\) at time step \(t\).</p> <p><strong>Off-policy</strong></p> <p>The training samples are generated by a stochastic policy \(\beta(a\vert s)\):</p> \[\begin{aligned} J_\beta(\theta) &amp;= \int_\mathcal{S} \rho^\beta Q^\mu(s, \mu_\theta(s)) ds \\ \nabla_\theta J_\beta(\theta) &amp;= \mathbb{E}_{s \sim \rho^\beta} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ] \end{aligned}\] <p>Example of off-policy actor-critic algorithm (Q-learning):</p> \[\begin{aligned} \delta_t &amp;= R_t + \gamma Q^w(s_{t+1}, \mu_\theta(s_{t+1})) - Q^w(s_t, a_t) &amp; \scriptstyle{\text{; TD error}}\\ w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) &amp; \\ \theta_{t+1} &amp;= \theta_t + \alpha_\theta {\nabla_a Q^w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} \end{aligned}\] <h3 id="ddpg">DDPG</h3> <p>Main work: combining the DPG with DQN</p> <p><img src="images/ddpg-algo.png" alt="DDPG"/></p> <h3 id="td3">TD3</h3> <p>Motivated by Double Q-learning and Double-DQN.</p> <p>Difference from DDPG:</p> <ol> <li> <p>Clipped Double Q-learning:</p> <ul> <li> <p>Two critics \(Q^{w_1}\) and \(Q^{w_2}\) as Double Q-learning and only one actor (actor and actor-target)</p> </li> <li> <p>Use the minimum estimation of two critics</p> \[y=r + \gamma \min_{i=1,2}Q^{w_i}(s', \mu_\theta(s'))\] </li> </ul> </li> <li> <p>Delayed update of Target and Policy Networks</p> </li> <li> <p>Target Policy smoothing</p> \[\begin{align*} y &amp;= r+\gamma Q_w(s', \mu_\theta(s')+\epsilon) \\ \epsilon &amp;\sim \text{clip}(\mathcal{N}(0, \sigma), -c, +c) \end{align*}\] </li> </ol> <p><img src="images/TD3.png" alt="TD3"/></p>]]></content><author><name>Ruoqi</name></author><category term="rl"/><summary type="html"><![CDATA[Policy gradient methods mentioned:]]></summary></entry></feed>